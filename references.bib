%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for 谷口尚平 at 2019-01-28 19:27:08 +0900 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{ResNet,
	Author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	Booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	Date-Added = {2019-01-28 19:27:00 +0900},
	Date-Modified = {2019-01-28 19:27:08 +0900},
	Pages = {770--778},
	Title = {Deep residual learning for image recognition},
	Year = {2016}}

@article{Ravi2016,
	Author = {Ravi, Sachin and Larochelle, Hugo},
	Date-Added = {2019-01-24 22:22:53 +0900},
	Date-Modified = {2019-01-24 22:23:04 +0900},
	Title = {Optimization as a model for few-shot learning},
	Year = {2016}}

@article{NP,
	Author = {Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J and Eslami, SM and Teh, Yee Whye},
	Date-Added = {2019-01-24 22:03:06 +0900},
	Date-Modified = {2019-01-24 22:03:17 +0900},
	Journal = {arXiv preprint arXiv:1807.01622},
	Title = {Neural processes},
	Year = {2018}}

@article{CNP,
	Author = {Garnelo, Marta and Rosenbaum, Dan and Maddison, Chris J and Ramalho, Tiago and Saxton, David and Shanahan, Murray and Teh, Yee Whye and Rezende, Danilo J and Eslami, SM},
	Date-Added = {2019-01-24 22:02:24 +0900},
	Date-Modified = {2019-01-24 22:02:38 +0900},
	Journal = {arXiv preprint arXiv:1807.01613},
	Title = {Conditional neural processes},
	Year = {2018}}

@article{Gordon2018,
	Author = {Gordon, Jonathan and Bronskill, John and Bauer, Matthias and Nowozin, Sebastian and Turner, Richard E},
	Date-Added = {2019-01-14 22:55:46 +0900},
	Date-Modified = {2019-01-14 22:58:11 +0900},
	Journal = {arXiv preprint arXiv:1805.09921},
	Title = {Meta-Learning Probabilistic Inference for Prediction},
	Year = {2018}}

@misc{gqn_dataset,
	Date-Added = {2019-01-14 19:58:51 +0900},
	Date-Modified = {2019-01-25 16:51:22 +0900},
	Howpublished = {\url{https://github.com/deepmind/gqn-datasets}},
	Keywords = {Datasets used to train Generative Query Networks (GQNs) in the `Neural Scene Representation and Rendering' paper.},
	Url = {https://github.com/deepmind/gqn-datasets},
	Year = {2018},
	Bdsk-Url-1 = {https://github.com/deepmind/gqn-datasets}}

@inproceedings{CLSTM,
	Author = {Xingjian, SHI and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-Kin and Woo, Wang-chun},
	Booktitle = {Advances in neural information processing systems},
	Date-Added = {2019-01-14 18:31:41 +0900},
	Date-Modified = {2019-01-14 18:31:50 +0900},
	Pages = {802--810},
	Title = {Convolutional LSTM network: A machine learning approach for precipitation nowcasting},
	Year = {2015}}

@article{maml,
	Author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	Date-Added = {2019-01-13 17:20:12 +0900},
	Date-Modified = {2019-01-13 17:20:17 +0900},
	Journal = {arXiv preprint arXiv:1703.03400},
	Title = {Model-agnostic meta-learning for fast adaptation of deep networks},
	Year = {2017}}

@article{Bengio2013,
	Author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	Date-Added = {2019-01-09 16:16:47 +0900},
	Date-Modified = {2019-01-09 16:16:54 +0900},
	Journal = {IEEE transactions on pattern analysis and machine intelligence},
	Number = {8},
	Pages = {1798--1828},
	Publisher = {IEEE},
	Title = {Representation learning: A review and new perspectives},
	Volume = {35},
	Year = {2013}}

@article{MNIST,
	Author = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
	Date-Added = {2019-01-09 16:12:41 +0900},
	Date-Modified = {2019-01-09 16:12:59 +0900},
	Journal = {Proceedings of the IEEE},
	Number = {11},
	Pages = {2278--2324},
	Publisher = {IEEE},
	Title = {Gradient-based learning applied to document recognition},
	Volume = {86},
	Year = {1998}}

@article{Eslami2018,
	Author = {Eslami, SM Ali and Rezende, Danilo Jimenez and Besse, Frederic and Viola, Fabio and Morcos, Ari S and Garnelo, Marta and Ruderman, Avraham and Rusu, Andrei A and Danihelka, Ivo and Gregor, Karol and others},
	Date-Added = {2019-01-09 16:10:20 +0900},
	Date-Modified = {2019-01-09 16:10:55 +0900},
	Journal = {Science},
	Number = {6394},
	Pages = {1204--1210},
	Publisher = {American Association for the Advancement of Science},
	Title = {Neural scene representation and rendering},
	Volume = {360},
	Year = {2018}}

@inproceedings{Gregor2016,
	Author = {Gregor, Karol and Besse, Frederic and Rezende, Danilo Jimenez and Danihelka, Ivo and Wierstra, Daan},
	Booktitle = {Advances In Neural Information Processing Systems},
	Date-Added = {2019-01-09 02:17:05 +0900},
	Date-Modified = {2019-01-09 02:17:15 +0900},
	Pages = {3549--3557},
	Title = {Towards conceptual compression},
	Year = {2016}}

@article{Gregor2015,
	Author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
	Date-Added = {2019-01-09 02:16:22 +0900},
	Date-Modified = {2019-01-09 02:16:34 +0900},
	Journal = {arXiv preprint arXiv:1502.04623},
	Title = {Draw: A recurrent neural network for image generation},
	Year = {2015}}

@article{Larsen2015,
	Author = {Larsen, Anders Boesen Lindbo and S{\o}nderby, S{\o}ren Kaae and Larochelle, Hugo and Winther, Ole},
	Date-Added = {2019-01-09 02:05:21 +0900},
	Date-Modified = {2019-01-09 02:05:32 +0900},
	Journal = {arXiv preprint arXiv:1512.09300},
	Title = {Autoencoding beyond pixels using a learned similarity metric},
	Year = {2015}}

@article{Mansimov2015,
	Author = {Mansimov, Elman and Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
	Date-Added = {2019-01-09 02:00:04 +0900},
	Date-Modified = {2019-01-09 02:00:22 +0900},
	Journal = {arXiv preprint arXiv:1511.02793},
	Title = {Generating images from captions with attention},
	Year = {2015}}

@incollection{Sohn2015,
	Author = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
	Booktitle = {Advances in Neural Information Processing Systems 28},
	Date-Added = {2019-01-09 01:19:39 +0900},
	Date-Modified = {2019-01-09 01:19:51 +0900},
	Editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
	Pages = {3483--3491},
	Publisher = {Curran Associates, Inc.},
	Title = {Learning Structured Output Representation using Deep Conditional Generative Models},
	Url = {http://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf},
	Year = {2015},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf}}

@article{Vilalta2002,
	Author = {Vilalta, Ricardo and Drissi, Youssef},
	Date-Added = {2019-01-07 01:13:00 +0900},
	Date-Modified = {2019-01-07 01:13:24 +0900},
	Journal = {Artificial Intelligence Review},
	Number = {2},
	Pages = {77--95},
	Publisher = {Springer},
	Title = {A perspective view and survey of meta-learning},
	Volume = {18},
	Year = {2002}}

@incollection{Ha2018,
	Author = {Ha, David and Schmidhuber, J\"{u}rgen},
	Booktitle = {Advances in Neural Information Processing Systems 31},
	Date-Added = {2019-01-05 16:09:44 +0900},
	Date-Modified = {2019-01-05 16:09:53 +0900},
	Editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	Pages = {2455--2467},
	Publisher = {Curran Associates, Inc.},
	Title = {Recurrent World Models Facilitate Policy Evolution},
	Url = {http://papers.nips.cc/paper/7512-recurrent-world-models-facilitate-policy-evolution.pdf},
	Year = {2018},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/7512-recurrent-world-models-facilitate-policy-evolution.pdf}}

@article{cite-key,
	Date-Added = {2019-01-04 23:40:34 +0900},
	Date-Modified = {2019-01-04 23:40:34 +0900}}

@article{Ganin2016,
	Acmid = {2946704},
	Author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran\c{c}ois and Marchand, Mario and Lempitsky, Victor},
	Date-Added = {2019-01-04 22:05:47 +0900},
	Date-Modified = {2019-01-04 22:06:05 +0900},
	Issn = {1532-4435},
	Issue_Date = {January 2016},
	Journal = {J. Mach. Learn. Res.},
	Keywords = {deep learning, domain adaptation, image classification, neural network, person re-identification, representation learning, sentiment analysis, synthetic data},
	Month = jan,
	Number = {1},
	Numpages = {-65},
	Pages = {2096--2030},
	Publisher = {JMLR.org},
	Title = {Domain-adversarial Training of Neural Networks},
	Url = {http://dl.acm.org/citation.cfm?id=2946645.2946704},
	Volume = {17},
	Year = {2016},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2946645.2946704}}

@article{French1999,
	Abstract = {All natural cognitive systems, and, in particular, our own, gradually forget previously learned information. Plausible models of human cognition should therefore exhibit similar patterns of gradual forgetting of old information as new information is acquired. Only rarely does new learning in natural cognitive systems completely disrupt or erase previously learned information; that is, natural cognitive systems do not, in general, forget `catastrophically'. Unfortunately, though, catastrophic forgetting does occur under certain circumstances in distributed connectionist networks. The very features that give these networks their remarkable abilities to generalize, to function in the presence of degraded input, and so on, are found to be the root cause of catastrophic forgetting. The challenge in this field is to discover how to keep the advantages of distributed connectionist networks while avoiding the problem of catastrophic forgetting. In this article the causes, consequences and numerous solutions to the problem of catastrophic forgetting in neural networks are examined. The review will consider how the brain might have overcome this problem and will also explore the consequences of this solution for distributed connectionist networks.},
	Author = {Robert M. French},
	Date-Added = {2019-01-04 20:09:17 +0900},
	Date-Modified = {2019-01-04 20:09:31 +0900},
	Doi = {https://doi.org/10.1016/S1364-6613(99)01294-2},
	Issn = {1364-6613},
	Journal = {Trends in Cognitive Sciences},
	Keywords = {Catastrophic forgetting, Connectionist networks, Connectionism, Memory, Learning, Interference},
	Number = {4},
	Pages = {128 - 135},
	Title = {Catastrophic forgetting in connectionist networks},
	Url = {http://www.sciencedirect.com/science/article/pii/S1364661399012942},
	Volume = {3},
	Year = {1999},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1364661399012942},
	Bdsk-Url-2 = {https://doi.org/10.1016/S1364-6613(99)01294-2}}

@article{Hornik1989,
	Abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
	Author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
	Date-Added = {2019-01-04 19:40:08 +0900},
	Date-Modified = {2019-01-04 19:40:18 +0900},
	Doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
	Issn = {0893-6080},
	Journal = {Neural Networks},
	Keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
	Number = {5},
	Pages = {359 - 366},
	Title = {Multilayer feedforward networks are universal approximators},
	Url = {http://www.sciencedirect.com/science/article/pii/0893608089900208},
	Volume = {2},
	Year = {1989},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/0893608089900208},
	Bdsk-Url-2 = {https://doi.org/10.1016/0893-6080(89)90020-8}}

@article{DL2015,
	Author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	Date = {2015/05/27/online},
	Date-Added = {2019-01-04 19:00:44 +0900},
	Date-Modified = {2019-01-04 19:00:44 +0900},
	Day = {27},
	Journal = {Nature},
	L3 = {10.1038/nature14539;},
	Month = {05},
	Pages = {436 EP -},
	Publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved. SN -},
	Title = {Deep learning},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/nature14539},
	Volume = {521},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1038/nature14539}}

@article{Hassabis2017,
	Abstract = {The fields of neuroscience and artificial intelligence (AI) have a long and intertwined history. In more recent times, however, communication and collaboration between the two fields has become less commonplace. In this article, we argue that better understanding biological brains could play a vital role in building intelligent machines. We survey historical interactions between the AI and neuroscience fields and emphasize current advances in AI that have been inspired by the study of neural computation in humans and other animals. We conclude by highlighting shared themes that may be key for advancing future research in both fields. Hassabis et al. review how neuroscience has informed research in artificial intelligence. They argue that a better understanding of biological brains will play a vital role in building intelligent machines.},
	Author = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
	File = {:Users/tmats/papers/1-s2.0-S0896627317305093-main.pdf:pdf},
	Journal = {Neuron},
	Keywords = {artificial intelligence,brain,cognition,learning,neural network},
	Number = {2},
	Pages = {245--258},
	Title = {{Neuroscience-Inspired Artificial Intelligence}},
	Volume = {95},
	Year = {2017}}

@incollection{Hinton2012,
	Abstract = {Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide how to set the values of numerical meta-parameters. Over the last few years, the machine learning group at the University of Toronto has acquired considerable expertise at training RBMs and this guide is an attempt to share this expertise with other machine learning researchers.},
	Address = {Berlin, Heidelberg},
	Author = {Hinton, Geoffrey E},
	Booktitle = {Neural Networks: Tricks of the Trade: Second Edition},
	Editor = {Montavon, Gr{\'{e}}goire and Orr, Genevi{\`{e}}ve B and M{\"{u}}ller, Klaus-Robert},
	Pages = {599--619},
	Publisher = {Springer Berlin Heidelberg},
	Title = {{A Practical Guide to Training Restricted Boltzmann Machines}},
	Year = {2012}}

@inproceedings{Mathieu2016,
	Abstract = {Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical flow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still, many vision applications could benefit from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectories. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset},
	Author = {Mathieu, Michael and Couprie, Camille and LeCun, Yann},
	Booktitle = {Proceedings of the 4th International Conference on Learning Representations (ICLR)},
	File = {:Users/tmats/papers/1511.05440.pdf:pdf},
	Title = {{Deep multi-scale video prediction beyond mean square error}},
	Year = {2016}}

@article{Sutton1999a,
	Abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the ¯rst time that a version of policy iteration with arbitrary di{\textregistered}erentiable function approximation is convergent to a locally optimal policy.},
	Author = {Sutton, Richard S. and Mcallester, David and Singh, Satinder and Mansour, Yishay},
	File = {:Users/tmats/papers/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf:pdf},
	Journal = {In Advances in Neural Information Processing Systems 12},
	Pages = {1057--1063},
	Title = {{Policy Gradient Methods for Reinforcement Learning with Function Approximation}},
	Year = {1999}}

@inproceedings{Houthooft2016,
	Abstract = {Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.},
	Author = {Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and {De Turck}, Filip and Abbeel, Pieter},
	Booktitle = {Advances in Neural Information Processing Systems 29},
	File = {:Users/tmats/papers/1605.09674.pdf:pdf},
	Pages = {1109--1117},
	Title = {{VIME: Variational Information Maximizing Exploration}},
	Year = {2016}}

@article{Montufar2016,
	Abstract = {Reinforcement learning for embodied agents is a challenging problem. The accumulated reward to be optimized is often a very rugged function, and gradient methods are impaired by many local optimizers. We demonstrate, in an experimental setting, that incorporating an intrinsic reward can smoothen the optimization landscape while preserving the global optimizers of interest. We show that policy gradient optimization for locomotion in a complex morphology is significantly improved when supplementing the extrinsic reward by an intrinsic reward defined in terms of the mutual information of time consecutive sensor readings.},
	Author = {Montufar, Guido and Ghazi-Zahedi, Keyan and Ay, Nihat},
	File = {:Users/tmats/papers/1605.09735.pdf:pdf},
	Keywords = {embod-,ied systems,intrinsic motivation,non-convex optimization,policy gradient,pomdp,predictive information,reinforcement learning},
	Title = {{Information Theoretically Aided Reinforcement Learning for Embodied Agents}},
	Year = {2016}}

@article{Friston2010,
	Author = {Friston, Karl},
	File = {:Users/tmats/papers/nrn2787.pdf:pdf},
	Journal = {Nature Reviews Neuroscience},
	Number = {2},
	Pages = {127--138},
	Publisher = {Nature Publishing Group},
	Title = {{The free-energy principle : a unified brain theory ?}},
	Volume = {11},
	Year = {2010}}

@article{Buckley2017,
	Abstract = {The `free energy principle' (FEP) has been suggested to provide a unified theory of the brain, integrating data and theory relating to action, perception, and learning. The theory and implementation of the FEP combines insights from Helmholtzian `perception as inference', machine learning theory, and statistical thermodynamics. Here, we provide a detailed mathematical evaluation of a suggested biologically plausible implementation of the FEP that has been widely used to develop the theory. Our objectives are (i) to describe within a single article the mathematical structure of this implementation of the FEP; (ii) provide a simple but complete agent-based model utilising the FEP and (iii) to disclose the assumption structure of this implementation of the FEP to help elucidate its significance for the brain sciences.},
	Author = {Buckley, Christopher L. and Kim, Chang Sub and McGregor, Simon and Seth, Anil K.},
	File = {:Users/tmats/papers/1-s2.0-S0022249617300962-main.pdf:pdf},
	Journal = {Journal of Mathematical Psychology},
	Keywords = {Action,Agent-based model,Bayesian brain,Free energy principle,Inference,Perception},
	Pages = {55--79},
	Title = {{The free energy principle for action and perception: A mathematical review}},
	Volume = {81},
	Year = {2017}}

@article{Brooks1991,
	Abstract = {Artificial intelligence research has foundered on the issue of representation. When intelligence is approached in an incremental manner, with strict reliance on interfacing to the real world through perception and action, reliance on representation disappears. In this paper we outline our approach to incrementally building complete intelligent Creatures. The fundamental decomposition of the intelligent system is not into independent information processing units which must interface with each other via representations. Instead, the intelligent system is decomposed into independent and parallel activity producers which all interface directly to the world through perception and action, rather than interface to each other particularly much. The notions of central and peripheral systems evaporate-everything is both central and peripheral. Based on these principles we have built a very successful series of mobile robots which operate without supervision as Creatures in standard office environments. ?? 1991.},
	Author = {Brooks, Rodney A.},
	File = {:Users/tmats/papers/representation.pdf:pdf},
	Journal = {Artificial Intelligence},
	Number = {1-3},
	Pages = {139--159},
	Title = {{Intelligence without representation}},
	Volume = {47},
	Year = {1991}}

@book{Doi2006,
	Abstract = {本体価格: 3500円 文献あり},
	Author = {土井利忠 and 藤田雅博 and 下村秀樹},
	Publisher = {丸善出版},
	Title = {身体を持つ知能：脳科学とロボティクスの共進化},
	Year = {2006}}

@article{Sommer2008,
	Abstract = {Each movement we make activates our own sensory receptors, thus causing a problem for the brain: the spurious, movement-related sensations must be discriminated from the sensory inputs that really matter, those representing our environment. Here we consider circuits for solving this problem in the primate brain. Such circuits convey a copy of each motor command, known as a corollary discharge (CD), to brain regions that use sensory input. In the visual system, CD signals may help to produce a stable visual percept from the jumpy images resulting from our rapid eye movements. A candidate pathway for providing CD for vision ascends from the superior colliculus to the frontal cortex in the primate brain. This circuit conveys warning signals about impending eye movements that are used for planning subsequent movements and analyzing the visual world. Identifying this circuit has provided a model for studying CD in other primate sensory systems and may lead to a better understanding of motor and mental disorders.},
	Author = {Sommer, Marc A. and Wurtz, Robert H.},
	File = {:Users/tmats/papers/sommer (2008) brain circuits for the internal monitoring of movements.pdf:pdf},
	Journal = {Annual Review of Neuroscience},
	Keywords = {corollary discharge,efference copy,saccadic eye movements,vision},
	Number = {1},
	Pages = {317--338},
	Title = {{Brain Circuits for the Internal Monitoring of Movements}},
	Volume = {31},
	Year = {2008}}

@article{Ryan2000,
	Abstract = {Intrinsic and extrinsic types of motivation have been widely studied, and the distinction between them has shed important light on both developmental and educational practices. In this review we revisit the classic definitions of intrinsic and extrinsic motivation in light of contemporary research and theory. Intrinsic motivation remains an important construct, reflecting the natural human propensity to learn and assimilate. However, extrinsic motivation is argued to vary considerably in its relative autonomy and thus can either reflect external control or true self-regulation. The relations of both classes of motives to basic human needs for autonomy, competence and relatedness are discussed. {\textcopyright} 2000 Academic Press.},
	Author = {Ryan, Richard M. and Deci, Edward L.},
	File = {:Users/tmats/papers/Ryan,+Deci+00.pdf:pdf},
	Journal = {Contemporary Educational Psychology},
	Number = {1},
	Pages = {54--67},
	Title = {{Intrinsic and Extrinsic Motivations: Classic Definitions and New Directions}},
	Volume = {25},
	Year = {2000}}

@article{Lopes2012,
	Abstract = {Formal exploration approaches in model-based reinforcement learning estimate the accuracy of the currently learned model without consideration of the empirical prediction error. For example, PAC-MDP approaches such as R-MAX base their model certainty on the amount of collected data, while Bayesian approaches assume a prior over the transition dynamics. We propose extensions to such approaches which drive exploration solely based on empirical estimates of the learner's accuracy and learning progress. We provide a 'sanity check' theoretical analysis, discussing the behavior of our extensions in the standard stationary finite state-action case. We then provide experimental studies demonstrating the robustness of these exploration measures in cases of non-stationary environments or where original approaches are misled by wrong domain assumptions.},
	Author = {Lopes, Manuel and Lang, Tobias and Toussaint, Marc and Oudeyer, Py},
	File = {:Users/tmats/papers/4642-exploration-in-model-based-reinforcement-learning-by-empirically-estimating-learning-progress.pdf:pdf},
	Journal = {{\ldots} Processing Systems (NIPS {\ldots}},
	Number = {i},
	Pages = {1--10},
	Title = {{Exploration in model-based reinforcement learning by empirically estimating learning progress}},
	Year = {2012}}

@book{2014,
	Author = {谷口忠大},
	Publisher = {講談社選書メチエ},
	Title = {記号創発ロボティクス},
	Year = {2014}}

@article{Uehara2016,
	Abstract = {Generative adversarial networks (GANs) are successful deep generative models. GANs are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats the density ratio estimation and f-divergence minimization. Our algorithm offers a new perspective toward the understanding of GANs and is able to make use of multiple viewpoints obtained in the research of density ratio estimation, e.g. what divergence is stable and relative density ratio is useful.},
	Author = {Uehara, Masatoshi and Sato, Issei and Suzuki, Masahiro and Nakayama, Kotaro and Matsuo, Yutaka},
	File = {:Users/tmats/papers/1610.02920.pdf:pdf},
	Title = {{Generative Adversarial Nets from a Density Ratio Estimation Perspective}},
	Year = {2016}}

@article{Goodfellow2016,
	Abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
	Author = {Goodfellow, Ian},
	File = {:Users/tmats/Library/Application Support/Mendeley Desktop/Downloaded/Goodfellow - 2016 - NIPS 2016 Tutorial Generative Adversarial Networks.pdf:pdf;:Users/tmats/papers/1701.00160.pdf:pdf},
	Title = {{NIPS 2016 Tutorial: Generative Adversarial Networks}},
	Year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxArLi4vLi4vLi4vLi4vRG93bmxvYWRzL1MxMzY0NjYxMzk5MDEyOTQyLmJpYk8RAWwAAAAAAWwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////xVTMTM2NDY2MTM5OTAxMjk0Mi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAQAAgAACiBjdQAAAAAAAAAAAAAAAAAJRG93bmxvYWRzAAACADIvOlVzZXJzOmlzaG9oZWkyMjA6RG93bmxvYWRzOlMxMzY0NjYxMzk5MDEyOTQyLmJpYgAOACwAFQBTADEAMwA2ADQANgA2ADEAMwA5ADkAMAAxADIAOQA0ADIALgBiAGkAYgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASADBVc2Vycy9pc2hvaGVpMjIwL0Rvd25sb2Fkcy9TMTM2NDY2MTM5OTAxMjk0Mi5iaWIAEwABLwAAFQACABH//wAAAAgADQAaACQAUgAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAHC}}

@article{Goodfellow2014a,
	Author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Joshua},
	File = {:Users/tmats/papers/08253599.pdf:pdf},
	Number = {January},
	Pages = {53--65},
	Title = {{Generative Adversarial Networks}},
	Year = {2014}}

@article{Ikegami2008,
	Abstract = {Life as an autonomous homeostatic system is discussed. A mechanism that drives a homeostatic state to an autonomous self-moving state is examined with two computational cell models. The mechanism is met with Ashby's ultrastability, where random parameter searching is activated when a system breaks a viability constraint. Such a random search process is replaced by the membrane shape in the first model and by chaotic population dynamics in the second model. Emergence of sensors, motors and the recursive coupling between them is shown to be a natural outcome of an autonomous homeostatic system. {\textcopyright} 2007 Elsevier Ireland Ltd. All rights reserved.},
	Author = {Ikegami, Takashi and Suzuki, Keisuke},
	File = {:Users/tmats/papers/Ikegami{\_}biosystems{\_}2008.pdf:pdf},
	Journal = {BioSystems},
	Keywords = {Autonomy,Autopoieisis,Chemotaxisis,Self-reproduction,Sensory-motor coupling},
	Number = {2},
	Pages = {388--400},
	Title = {{From a homeostatic to a homeodynamic self}},
	Volume = {91},
	Year = {2008}}

@article{Momennejad2017,
	Abstract = {Theories of reinforcement learning in neuroscience have focused on two families of algorithms. Model-free algorithms cache action values, making them cheap but inflexible: a candidate mechanism for adaptive and maladaptive habits. Model-based algorithms achieve flexibility at computational expense, by rebuilding values from a model of the environment. We examine an intermediate class of algorithms, the successor representation (SR), which caches long-run state expectancies, blending model-free efficiency with model-based flexibility. Although previous reward revaluation studies distinguish model-free from model-based learning algorithms, such designs cannot discriminate between model-based and SR-based algorithms, both of which predict sensitivity to reward revaluation. However, changing the transition structure (" transition revaluation ") should selectively impair revaluation for the SR. In two studies we provide evidence that humans are differentially sensitive to reward vs. transition revaluation, consistent with SR predictions. These results support a new neuro-computational mechanism for flexible choice, while introducing a subtler, more cognitive notion of habit.},
	Author = {Momennejad, I. and Russek, E. M. and Cheong, J. H. and Botvinick, M. M. and Daw, N. D. and Gershman, S. J.},
	File = {:Users/tmats/papers/083824.full.pdf:pdf},
	Journal = {Nature Human Behaviour},
	Keywords = {decision making,human behavior,model-based,planning,predictive representation,reinforcement learning,retrospective revaluation,successor representation},
	Number = {9},
	Pages = {680--692},
	Title = {{The successor representation in human reinforcement learning}},
	Volume = {1},
	Year = {2017}}

@article{Stollenga2014,
	Abstract = {Traditional convolutional neural networks (CNN) are stationary and feedforward. They neither change their parameters during evaluation nor use feedback from higher to lower layers. Real brains, however, do. So does our Deep Attention Selective Network (dasNet) architecture. DasNets feedback structure can dynamically alter its convolutional filter sensitivities during classification. It harnesses the power of sequential processing to improve classification performance, by allowing the network to iteratively focus its internal attention on some of its convolutional filters. Feedback is trained through direct policy search in a huge million-dimensional parameter space, through scalable natural evolution strategies (SNES). On the CIFAR-10 and CIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model.},
	Author = {Stollenga, Marijn and Masci, Jonathan and Gomez, Faustino and Schmidhuber, Juergen},
	File = {:Users/tmats/papers/1407.3068.pdf:pdf},
	Pages = {1--13},
	Title = {{Deep Networks with Internal Selective Attention through Feedback Connections}},
	Year = {2014}}

@article{Keramati2014,
	Abstract = {Efficient regulation of internal homeostasis and defending it against perturbations requires adaptive behavioral strategies. However, the computational principles mediating the interaction between homeostatic and associative learning processes remain undefined. Here we use a definition of primary rewards, as outcomes fulfilling physiological needs, to build a normative theory showing how learning motivated behaviors may be modulated by internal states. Within this framework, we mathematically prove that seeking rewards is equivalent to the fundamental objective of physiological stability, defining the notion of physiological rationality of behavior. We further suggest a formal basis for temporal discounting of rewards by showing that discounting motivates animals to follow the shortest path in the space of physiological variables toward the desired setpoint. We also explain how animals learn to act predictively to preclude prospective homeostatic challenges, and several other behavioral patterns. Finally, we suggest a computational role for interaction between hypothalamus and the brain reward system.},
	Author = {Keramati, Mehdi and Gutkin, Boris},
	File = {:Users/tmats/papers/elife-04811-v1.pdf:pdf},
	Journal = {eLife},
	Keywords = {anticipatory responding,cortico-basal ganglia,homeostatic regulation,hypothalamus,neuroscience,none,reinforcement learning,temporal discounting},
	Pages = {1--26},
	Title = {{Homeostatic reinforcement learning for integrating reward collection and physiological stability}},
	Volume = {3},
	Year = {2014}}

@article{Buhrmann2013,
	Abstract = {According to the sensorimotor approach, perception is a form of embodied know-how, constituted by lawful regularities in the sensorimotor flow or in sensorimotor contingencies (SMCs) in an active and situated agent. Despite the attention that this approach has attracted, there have been few attempts to define its core concepts formally. In this paper, we examine the idea of SMCs and argue that its use involves notions that need to be distinguished. We introduce four distinct kinds of SMCs, which we define operationally. These are the notions of sensorimotor environment (open-loop motor-induced sensory variations), sensorimotor habitat (closed-loop sensorimotor trajectories), sensorimotor coordination (reliable sensorimotor patterns playing a functional role), and sensorimotor strategy (normative organization of sensorimotor coordinations). We make use of a minimal dynamical model of visually guided categorization to test the explanatory value of the different kinds of SMCs. Finally, we discuss the impact of our definitions on the conceptual development and empirical as well as model-based testing of the claims of the sensorimotor approach.},
	Author = {Buhrmann, Thomas and {Di Paolo}, Ezequiel Alejandro and Barandiaran, Xabier},
	File = {:Users/tmats/papers/fpsyg-04-00285.pdf:pdf},
	Journal = {Frontiers in Psychology},
	Keywords = {Dynamical systems,Embodied cognition,Minimal cognition,Sensorimotor approach to perception,Sensorimotor contingencies},
	Number = {MAY},
	Pages = {1--19},
	Title = {{A dynamical systems account of sensorimotor contingencies}},
	Volume = {4},
	Year = {2013}}

@article{Gal2016,
	Abstract = {Deep learning has attracted tremendous attention from researchers in various fields of information engineering such as AI, computer vision, and language processing [Kalchbrenner and Blunsom, 2013; Krizhevsky et al., 2012; Mnih et al., 2013], but also from more traditional sciences such as physics, biology, and manufacturing [Anjos et al., 2015; Baldi et al., 2014; Bergmann et al., 2014]. Neural networks, image processing tools such as convolutional neural networks, sequence processing models such as recurrent neural networks, and regularisation tools such as dropout, are used extensively. However, fields such as physics, biology, and manufacturing are ones in which representing model uncertainty is of crucial importance [Ghahramani, 2015; Krzywinski and Altman, 2013]. With the recent shift in many of these fields towards the use of Bayesian uncertainty [Herzog and Ostwald, 2013; Nuzzo, 2014; Trafimow and Marks, 2015], new needs arise from deep learning. In this work we develop tools to obtain practical uncertainty estimates in deep learning, casting recent deep learning tools as Bayesian models without changing either the models or the optimisation. In the first part of this thesis we develop the theory for such tools, providing applications and illustrative examples. We tie approximate inference in Bayesian models to dropout and other stochastic regularisation techniques, and assess the approximations empirically. We give example applications arising from this connection between modern deep learning and Bayesian modelling such as active learning of image data and data efficient deep reinforcement learning. We further demonstrate the method's practicality through a survey of recent applications making use of the suggested tools in language applications, medical diagnostics, bioinformatics, image processing, and autonomous driving. In the second part of the thesis we explore its theoretical implications, and the insights stemming from the link between Bayesian modelling and deep learning. We discuss what determines model uncertainty properties, analyse the approximate inference analytically in the linear case, and theoretically examine various priors such as spike and slab priors.},
	Author = {Gal, Yarin},
	File = {:Users/tmats/papers/Uncertainty in Deep Learning.pdf:pdf},
	Number = {October},
	Pages = {174},
	Title = {{Uncertainty in Deep Learning}},
	Year = {2016}}

@article{Pfau2016,
	Abstract = {Both generative adversarial networks (GAN) in unsupervised learning and actor-critic methods in reinforcement learning (RL) have gained a reputation for being difficult to optimize. Practitioners in both fields have amassed a large number of strategies to mitigate these instabilities and improve training. Here we show that GANs can be viewed as actor-critic methods in an environment where the actor cannot affect the reward. We review the strategies for stabilizing training for each class of models, both those that generalize between the two and those that are particular to that model. We also review a number of extensions to GANs and RL algorithms with even more complicated information flow. We hope that by highlighting this formal connection we will encourage both GAN and RL communities to develop general, scalable, and stable algorithms for multilevel optimization with deep networks, and to draw inspiration across communities.},
	Author = {Pfau, David and Vinyals, Oriol},
	File = {:Users/tmats/papers/1610.01945.pdf:pdf},
	Title = {{Connecting Generative Adversarial Networks and Actor-Critic Methods}},
	Year = {2016}}

@article{Bengio2017,
	Abstract = {A new prior is proposed for representation learning, which can be combined with other priors in order to help disentangling abstract factors from each other. It is inspired by the phenomenon of consciousness seen as the formation of a low-dimensional combination of a few concepts constituting a conscious thought, i.e., consciousness as awareness at a particular time instant. This provides a powerful constraint on the representation in that such low-dimensional thought vectors can correspond to statements about reality which are true, highly probable, or very useful for taking decisions. The fact that a few elements of the current state can be combined into such a predictive or useful statement is a strong constraint and deviates considerably from the maximum likelihood approaches to modelling data and how states unfold in the future based on an agent's actions. Instead of making predictions in the sensory (e.g. pixel) space, the consciousness prior allows the agent to make predictions in the abstract space, with only a few dimensions of that space being involved in each of these predictions. The consciousness prior also makes it natural to map conscious states to natural language utterances or to express classical AI knowledge in the form of facts and rules, although the conscious states may be richer than what can be expressed easily in the form of a sentence, a fact or a rule.},
	Archiveprefix = {arXiv},
	Arxivid = {1709.08568},
	Author = {Bengio, Yoshua},
	Eprint = {1709.08568},
	File = {:Users/tmats/papers/1709.08568.pdf:pdf},
	Number = {1},
	Pages = {1--4},
	Title = {{The Consciousness Prior}},
	Url = {http://arxiv.org/abs/1709.08568},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1709.08568}}

@article{Keramati2011,
	Abstract = {Reinforcement learning models address animal's behavioral adaptation to its changing ``external'' environment, and are based on the assumption that Pavlo- vian, habitual and goal-directed responses seek to maximize reward acquisition. Negative-feedback models of homeostatic regulation, on the other hand, are con- cerned with behavioral adaptation in response to the ``internal'' state of the animal, and assume that animals' behavioral objective is to minimize deviations of some key physiological variables from their hypothetical setpoints. Building upon the drive-reduction theory of reward, we propose a new analytical framework that in- tegrates learning and regulatory systems, such that the two seemingly unrelated objectives of reward maximization and physiological-stability prove to be identi- cal. The proposed theory shows behavioral adaptation to both internal and external states in a disciplined way. We further show that the proposed framework allows for a unified explanation of some behavioral pattern like motivational sensitivity of different associative learning mechanism, anticipatory responses, interaction among competing motivational systems, and risk aversion.},
	Author = {Keramati, Mehdi and Gutkin, Boris},
	File = {:Users/tmats/papers/4437-a-reinforcement-learning-theory-for-homeostatic-regulation.pdf:pdf},
	Isbn = {9781618395993},
	Journal = {Nips},
	Pages = {82--90},
	Title = {{A Reinforcement Learning theory for homeostatic regulation}},
	Year = {2011}}

@article{Sutton2017,
	Author = {Sutton, Richard S and Barto, Andrew G},
	File = {:Users/tmats/papers/bookdraft2018jan1.pdf:pdf},
	Title = {{Reinforcement Learning : An Introduction **** Complete Draft ****}},
	Year = {2017}}

@article{Froese2010,
	Abstract = {The concept of autopoiesis was conceived by Maturana and Varela as providing the necessary and sufficient conditions for distinguishing the living from the non-living (and, by extension, the cognitive from the non-cognitive). More recently however, there has been a growing consensus that their original conception of autopoiesis is necessary but insufficient for this task as it fails to meet a number of constructive, interactive, normative, and historical requirements. We argue that it also fails to satisfy crucial phenomenological requirements that are motivated by the ongoing appropriation of autopoiesis as a key concept in enactive cognitive science. The root of these problems can be traced to the abstract general systems framework in which the ideas were first formulated, as epitomized by Ashby's cybernetics. While this abstract generality has helped the concept's popularity in some circles, we insist that a restriction of autopoiesis to a radical embodiment in chemical self-production under far-from-equilibrium conditions is necessary if the concept is to live up to its original intentions.},
	Author = {Froese, T},
	File = {:Users/tmats/papers/308f748fc5a954e903c53cfdfd18c3ab0291.pdf:pdf},
	Journal = {Cybernetics {\&} Human Knowing},
	Number = {4},
	Pages = {7--50},
	Title = {{Life after Ashby: Ultrastability and the autopoietic foundations of biological autonomy}},
	Volume = {17},
	Year = {2010}}

@inproceedings{Kingma2014,
	Abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	Author = {Kingma, Diederik P. and Ba, Jimmy},
	Booktitle = {Proceedings of the 3rd International Conference on Learning Representations (ICLR)},
	File = {:Users/tmats/papers/1412.6980v8.pdf:pdf},
	Title = {{Adam: A Method for Stochastic Optimization}},
	Year = {2014}}

@inproceedings{Gregor2015a,
	Abstract = {This paper introduces the Deep Recurrent Atten- tive Writer (DRAW) neural network architecture for image generation. DRAWnetworks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distin- guished from real data with the naked eye. 1.},
	Author = {Gregor, K and Danihelka, I and Graves, A and Wierstra, D},
	Booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
	File = {:Users/tmats/papers/1502.04623.pdf:pdf},
	Pages = {1462--1471},
	Title = {{DRAW: A Recurrent Neural Network For Image Generation}},
	Year = {2015}}

@article{Shibata2017,
	Author = {柴田克成 and 後藤祐樹},
	File = {:Users/tmats/papers/24{\_}96.pdf:pdf},
	Journal = {認知科学},
	Number = {1},
	Pages = {96--117},
	Title = {深層学習が示唆するend-to-end強化学習に基づく 機能創発アプローチの重要性と思考の創発に向けたカオスニューラルネットを用いた新しい強化学習},
	Volume = {24},
	Year = {2017}}

@article{Katsuki2014,
	Abstract = {The brain is limited in its capacity to process all sensory stimuli present in the physical world at any point in time and relies instead on the cognitive process of attention to focus neural resources according to the contingencies of the moment. Attention can be categorized into two distinct functions: bottom-up attention, referring to attentional guidance purely by externally driven factors to stimuli that are salient because of their inherent properties relative to the background; and top-down attention, referring to internal guidance of attention based on prior knowledge, willful plans, and current goals. Over the past few years, insights on the neural circuits and mechanisms of bottom-up and top-down attention have been gained through neurophysiological experiments. Attention affects the mean neuronal firing rate as well as its variability and correlation across neurons. Although distinct processes mediate the guidance of attention based on bottom-up and top-down factors, a common neural apparatus, the frontoparietal network, is essential in both types of attentional processes.},
	Author = {Katsuki, Fumi and Constantinidis, Christos},
	File = {:Users/tmats/papers/1073858413514136.pdf:pdf},
	Journal = {Neuroscientist},
	Keywords = {intraparietal sulcus,monkey,neurophysiology,posterior parietal,prefrontal},
	Number = {5},
	Pages = {509--521},
	Title = {{Bottom-up and top-down attention: Different processes and overlapping neural systems}},
	Volume = {20},
	Year = {2014}}

@article{Georgeon2013,
	Abstract = {A novel way to model an agent interacting with an environment is introduced, called an Enactive Markov Decision Process (EMDP). An EMDP keeps perception and action embedded within sensorimotor schemes rather than dissociated. Instead of seeking a goal associated with a reward, as in reinforcement learning, an EMDP agent is driven by two forms of self-motivation: successfully enacting sequences of interactions (autotelic motivation), and preferably enacting interactions that have predefined positive values (interactional motivation). An EMDP learning algorithm is presented. Results show that the agent develops a rudimentary form of self-programming, along with active perception as it learns to master the sensorimotor contingencies afforded by its coupling with the environment.},
	Author = {Georgeon, Olivier L. and Wolf, Christian and Gay, Simon},
	File = {:Users/tmats/papers/GeorgeonO-Epirob2013-6.pdf:pdf},
	Journal = {2013 IEEE 3rd Joint International Conference on Development and Learning and Epigenetic Robotics, ICDL 2013 - Electronic Conference Proceedings},
	Keywords = {Enaction,constructivist learning,self-motivation},
	Title = {{An Enactive approach to autonomous agent and robot learning}},
	Year = {2013}}

@article{Zeleny1981,
	Author = {Zeleny, M},
	File = {:Users/tmats/papers/1194.pdf:pdf},
	Isbn = {0-444-00385-1},
	Journal = {Autopoiesis: a theory of living organization},
	Pages = {4--17},
	Title = {{What is Autopoiesis?}},
	Year = {1981}}

@article{Schumacher2008,
	Abstract = {This paper focuses on the epistemic conditions of visual perception, ie it concentrates on the question of what kind of knowledge is required for us in order to be able to see colours and shapes as spatial properties of things. According to contemporary theories of sensory perception that follow the tradition of George Berkeley, like Alva Noe's so-called enactive approach to perception, this type of visual perception requires a certain kind of implicit practical knowledge, namely implicit sensorimotor knowledge of the way sensory stimulation varies as the perceiver moves. Two objections are presented against this central claim of the enactive approach. First, empirical evidence from psychological research on children's cognitive and motor development suggests that visual content is entirely independent of sensorimotor knowledge. Second, the enactive approach gets involved in the characteristic problems of classical sense-datum theories by introducing the extremely problematic claim that the recognition of appearances is the epistemic starting point for the perception of things and their properties.},
	Author = {Schumacher, Ralph},
	File = {:Users/tmats/papers/p5878.pdf:pdf},
	Journal = {Perception},
	Number = {3},
	Pages = {433--445},
	Title = {{Action in perception? Empirical and philosophical arguments against the enactive approach to perception}},
	Volume = {37},
	Year = {2008}}

@article{Barandiaran2017,
	Abstract = {The concept of ``autonomy'', once at the core of the original enactivist proposal in The Embodied Mind (Varela et al. in The embodied mind: cognitive science and human experience. MIT Press, Cambridge, 1991), is nowadays ignored or neglected by some of the most prominent contemporary enactivists approaches. Theories of autonomy, however, come to fill a theoretical gap that sensorimotor accounts of cognition cannot ignore: they provide a naturalized account of normativity and the resources to ground the identity of a cognitive subject in its specific mode of organization. There are, however, good reasons for the contemporary neglect of autonomy as a relevant concept for enactivism. On the one hand, the concept of autonomy has too often been assimilated into autopoiesis (or basic autonomy in the molecular or biological realm) and the implications are not always clear for a dynamical sensorimotor approach to cognitive science. On the other hand, the foundational enactivist proposal displays a metaphysical tension between the concept of operational closure (autonomy), deployed as constitutive, and that of structural coupling (sensorimotor dynamics); making it hard to reconcile with the claim that experience is sensorimotorly constituted. This tension is particularly apparent when Varela et al. propose Bittorio (a 1D cellular automata) as a model of the operational closure of the nervous system as it fails to satisfy the required conditions for a sensorimotor constitution of experience. It is, however, possible to solve these problems by re-considering autonomy at the level of sensorimotor neurodynamics. Two recent robotic simulation models are used for this task, illustrating the notion of strong sensorimotor dependency of neurodynamic patterns, and their networked intertwinement. The concept of habit is proposed as an enactivist building block for cognitive theorizing, re-conceptualizing mental life as a habit ecology, tied within an agent's behaviour generating mechanism in coordination with its environment. Norms can be naturalized in terms of dynamic, interactively self-sustaining, coherentism. This conception of autonomous sensorimotor agency is put in contrast with those enactive approaches that reject autonomy or neglect the theoretical resources it has to offer for the project of naturalizing minds.},
	Author = {Barandiaran, Xabier E.},
	File = {:Users/tmats/papers/barandiaran{\_}-{\_}2016{\_}-{\_}autonomy{\_}and{\_}enactivism{\_}-{\_}topoi{\_}final.pdf:pdf},
	Journal = {Topoi},
	Keywords = {Autonomist sensorimotor enactivism,Autonomy,Enactivism,Habit,Mental life,Operational closure of the nervous system,Sensorimotor autonomous agency,Sensorimotor constitution of experience},
	Number = {3},
	Pages = {409--430},
	Publisher = {Springer Netherlands},
	Title = {{Autonomy and Enactivism: Towards a Theory of Sensorimotor Autonomous Agency}},
	Volume = {36},
	Year = {2017}}

@article{Shimonishi2001,
	Author = {Shimonishi, Kazeto},
	File = {:Users/tmats/papers/89{\_}6.pdf:pdf},
	Pages = {83--98},
	Title = {{― フランシスコ・ヴァレラのエナクティブ主義と現象学 ― Action in Life and Consciousness ：Fransisco Varela's Enactivism and Phenomenology}},
	Year = {2001}}

@article{Marcus2018,
	Author = {Marcus, Gary},
	File = {:Users/tmats/papers/1801.00631.pdf:pdf},
	Pages = {1--27},
	Title = {{Deep Learning : A Critical Appraisal}}}

@article{Luc2017,
	Abstract = {The ability to predict and therefore to anticipate the future is an important attribute of intelligence. It is also of utmost importance in real-time systems, e.g. in robotics or autonomous driving, which depend on visual scene understanding for decision making. While prediction of the raw RGB pixel values in future video frames has been studied in previous work, here we introduce the novel task of predicting semantic segmentations of future frames. Given a sequence of video frames, our goal is to predict segmentation maps of not yet observed video frames that lie up to a second or further in the future. We develop an autoregressive convolutional neural network that learns to iteratively generate multiple frames. Our results on the Cityscapes dataset show that directly predicting future segmentations is substantially better than predicting and then segmenting future RGB frames. Prediction results up to half a second in the future are visually convincing and are much more accurate than those of a baseline based on warping semantic segmentations using optical flow.},
	Archiveprefix = {arXiv},
	Arxivid = {1703.07684},
	Author = {Luc, Pauline and Neverova, Natalia and Couprie, Camille and Verbeek, Jakob and LeCun, Yann},
	Doi = {10.1109/ICCV.2017.77},
	Eprint = {1703.07684},
	File = {:Users/tmats/papers/1703.07684.pdf:pdf},
	Title = {{Predicting Deeper into the Future of Semantic Segmentation}},
	Url = {http://arxiv.org/abs/1703.07684},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1703.07684},
	Bdsk-Url-2 = {https://doi.org/10.1109/ICCV.2017.77}}

@book{Ueda2007,
	Abstract = {執筆:上田完次ほか 本体価格: 3700円 文献あり},
	Author = {上田完次 and 尾田十八 and 三木光範},
	Publisher = {培風館},
	Title = {創発とマルチエージェントシステム},
	Year = {2007}}

@book{Russell2003,
	Abstract = {Second edition. Presents a guide to artificial intelligence, covering such topics as intelligent agents, problem-solving, logical agents, planning, uncertainty, learning, and robotics. Artificial Intelligence -- Problem-solving -- Knowledge and reasoning -- Planning -- Uncertain knowledge and reasoning -- Learning -- Communicating, perceiving, and acting -- Conclusions.},
	Author = {Russell, Stuart J. and Norvig, Peter and Canny, John.},
	Pages = {1081},
	Publisher = {Pearson Education},
	Title = {{Artificial intelligence : a modern approach}},
	Year = {2003}}

@article{Brooks1986,
	Abstract = {A new architecture for controlling mobile robots is described. Layers of control system are built to let the robot operate at increasing levels of competence. Layers are made up of asynchronous modules that communicate over low-bandwidth channels. Each module is an instance of a fairly simple computational machine. Higher-level layers can subsume the roles of lower levels by suppressing their outputs. However, lower levels continue to function as higher levels are added. The result is a robust and flexible robot control system. The system has been used to control a mobile robot wandering around unconstrained laboratory areas and computer machine rooms. Eventually it is intended to control a robot that wanders the office areas of our laboratory, building maps of its surroundings using an onboard arm to perform simple tasks.},
	Author = {Brooks, Rodney A.},
	File = {:Users/tmats/papers/01087032.pdf:pdf},
	Journal = {IEEE Journal on Robotics and Automation},
	Number = {1},
	Pages = {14--23},
	Title = {{A Robust Layered Control System For A Mobile Robot}},
	Volume = {2},
	Year = {1986}}

@article{Silver2017,
	Abstract = {Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games.},
	Author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {Van Den Driessche}, George and Graepel, Thore and Hassabis, Demis},
	File = {:Users/tmats/papers/nature24270.pdf:pdf},
	Journal = {Nature},
	Number = {7676},
	Pages = {354--359},
	Title = {{Mastering the game of Go without human knowledge}},
	Volume = {550},
	Year = {2017}}

@article{Silver2016,
	Abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	Author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {Van Den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	File = {:Users/tmats/papers/AlphaGoNaturePaper.pdf:pdf},
	Journal = {Nature},
	Number = {7587},
	Pages = {484--489},
	Title = {{Mastering the game of Go with deep neural networks and tree search}},
	Volume = {529},
	Year = {2016}}

@article{Mescheder2017,
	Abstract = {Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.},
	Author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
	File = {:Users/tmats/papers/1701.04722.pdf:pdf},
	Title = {{Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks}},
	Year = {2017}}

@book{Clark1997,
	Abstract = {"A Bradford book." Preface : deep thought meets fluent action -- Acknowledgments -- Groundings -- Introduction : a car with a cockroach brain -- 1. Autonomous agents : walking on the moon -- 2. The situated infant -- 3. Mind and world : the plastic frontier -- 4. Collective wisdom, slime-mold-style -- 5. Evolving robots -- 6. Emergence and explanation -- 7. The neuroscientific image -- 8. Being, computing, representing -- 9. Minds and markets -- 10. Language : the ultimate artifact -- 11. Minds, brains, and tuna : a summary in brine -- Epilogue : a brain speaks.},
	Author = {Clark, Andy},
	Pages = {269},
	Publisher = {MIT Press},
	Title = {{Being there : putting brain, body, and world together again}},
	Year = {1997}}

@article{Kaelbling1998,
	Abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (MDPS) and partially observable MDPS (POMDPS). We then outline a novel algorithm for solving POMDPS off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to POMDPS, and of some possibilities for finding approximate solutions. 0 1998 Consider the problem of a robot navigating in a large office building. The robot can move from hallway intersection to intersection and can make local observations of its world. Its actions are not completely reliable, however. Sometimes, when it intends to move, it stays where it is or goes too far; sometimes, when it intends to turn, it overshoots. It has similar problems with observation. Sometimes a corridor looks like a comer; sometimes a T-junction looks like an L-junction. How can such an error-plagued robot navigate, even given a map of the corridors? In general, the robot will have to remember something about its history of actions and observations and use this information, together with its knowledge of the underlying dynamics of the world (the map and other information), to maintain an estimate of its location. Many engineering applications follow this approach, using methods like the Kalman filter [26] to maintain a running estimate of the robot's spatial uncertainty, expressed as an ellipsoid or normal distribution in Cartesian space. This approach will not do for our robot, though. Its uncertainty may be discrete: it might be almost certain that it is in the northeast comer of either the fourth or the seventh floors, though it admits a chance that it is on the fifth floor, as well. Then, given an uncertain estimate of its location, the robot has to decide what actions to take. In some cases, it might be sufficient to ignore its uncertainty and take actions that would be appropriate for the most likely location. In other cases, it might be better for the robot to take actions for the purpose of gathering information, such as searching for a landmark or reading signs on the wall. In general, it will take actions that fulfill both purposes simultaneously.},
	Author = {Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra, Anthony R.},
	File = {:Users/tmats/papers/aij98-pomdp.pdf:pdf},
	Journal = {Artificial Intelligence},
	Keywords = {a large office building,a robot navigating in,completely reliable,consider the problem of,from hallway intersection to,however,intersection and can make,it,its actions are not,local observations of its,move,partially observable markov decision,planning,processes,sometimes,the robot can move,uncertainty,when it intends to,world},
	Number = {1-2},
	Pages = {99--134},
	Title = {{Planning and acting in partially observable stochastic domains}},
	Volume = {101},
	Year = {1998}}

@inproceedings{Sutton1999,
	Abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the ¯rst time that a version of policy iteration with arbitrary di{\textregistered}erentiable function approximation is convergent to a locally optimal policy.},
	Author = {Sutton, Richard S. and Mcallester, David and Singh, Satinder and Mansour, Yishay},
	Booktitle = {Advances in Neural Information Processing Systems 12},
	File = {:Users/tmats/papers/PolicyGradient.pdf:pdf},
	Pages = {1057--1063},
	Title = {{Policy Gradient Methods for Reinforcement Learning with Function Approximation}},
	Year = {1999}}

@article{Butko2009,
	Abstract = {Recent years have seen the development of fast and accurate algorithms for detecting objects in images. However, as the size of the scene grows, so do the running-times of these algorithms. If a 128times102 pixel image requires 20 ms to process, searching for objects in a 1280times1024 image will take 2 s. This is unsuitable under real-time operating constraints: by the time a frame has been processed, the object may have moved. An analogous problem occurs when controlling robot camera that need to scan scenes in search of target objects. In this paper, we consider a method for improving the run-time of general-purpose object-detection algorithms. Our method is based on a model of visual search in humans, which schedules eye fixations to maximize the long-term information accrued about the location of the target of interest. The approach can be used to drive robot cameras that physically scan scenes or to improve the scanning speed for very large high resolution images. We consider the latter application in this work by simulating a ldquodigital foveardquo and sequentially placing it in various regions of an image in a way that maximizes the expected information gain. We evaluate the approach using the OpenCV version of the Viola-Jones face detector. After accounting for all computational overhead introduced by the fixation controller, the approach doubles the speed of the standard Viola-Jones detector at little cost in accuracy.},
	Author = {Butko, Nicholas J. and Movellan, Javier R.},
	Doi = {10.1109/CVPRW.2009.5206540},
	File = {:Users/tmats/papers/05206540.pdf:pdf},
	Issn = {1063-6919},
	Journal = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009},
	Number = {1},
	Pages = {2751--2758},
	Title = {{Optimal scanning for faster object detection}},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/CVPRW.2009.5206540}}

@article{Teh2017,
	Abstract = {Most deep reinforcement learning algorithms are data inefficient in complex and rich environments, limiting their applicability to many scenarios. One direction for improving data efficiency is multitask learning with shared neural network parameters, where efficiency may be improved through transfer across related tasks. In practice, however, this is not usually observed, because gradients from different tasks can interfere negatively, making learning unstable and sometimes even less data efficient. Another issue is the different reward schemes between tasks, which can easily lead to one task dominating the learning of a shared model. We propose a new approach for joint training of multiple tasks, which we refer to as Distral (Distill {\&} transfer learning). Instead of sharing parameters between the different workers, we propose to share a "distilled" policy that captures common behaviour across tasks. Each worker is trained to solve its own task while constrained to stay close to the shared policy, while the shared policy is trained by distillation to be the centroid of all task policies. Both aspects of the learning process are derived by optimizing a joint objective function. We show that our approach supports efficient transfer on complex 3D environments, outperforming several related methods. Moreover, the proposed learning process is more robust and more stable---attributes that are critical in deep reinforcement learning.},
	Archiveprefix = {arXiv},
	Arxivid = {1707.04175},
	Author = {Teh, Yee Whye and Bapst, Victor and Czarnecki, Wojciech Marian and Quan, John and Kirkpatrick, James and Hadsell, Raia and Heess, Nicolas and Pascanu, Razvan},
	Eprint = {1707.04175},
	File = {:Users/tmats/papers/1707.04175.pdf:pdf},
	Title = {{Distral: Robust Multitask Reinforcement Learning}},
	Url = {http://arxiv.org/abs/1707.04175},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1707.04175}}

@article{Butko2008,
	Abstract = {Modeling eye-movements during search is important for building intelligent robotic vision systems, and for understanding how humans select relevant information and structure behavior in real time. Previous models of visual search (VS) rely on the idea of ldquosaliency mapsrdquo which indicate likely locations for targets of interest. In these models the eyes move to locations with maximum saliency. This approach has several drawbacks: (1) It assumes that oculomotor control is a greedy process, i.e., every eye movement is planned as if no further eye movements would be possible after it. (2) It does not account for temporal dynamics and how information is integrated as over time. (3) It does not provide a formal basis to understand how optimal search should vary as a function of the operating characteristics of the visual system. To address these limitations, we reformulate the problem of VS as an Information-gathering Partially Observable Markov Decision Process (I-POMDP). We find that the optimal control law depends heavily on the Foveal-Peripheral Operating Characteristic (FPOC) of the visual system.},
	Author = {Butko, Nicholas J. and Movellan, Javier R.},
	File = {:Users/tmats/papers/04640819.pdf:pdf},
	Journal = {2008 IEEE 7th International Conference on Development and Learning, ICDL},
	Number = {1},
	Pages = {139--144},
	Title = {{I-POMDP: An infomax model of eye movement}},
	Year = {2008}}

@book{Schwab2016,
	Abstract = {Professor Klaus Schwab, Founder and Executive Chairman of the World Economic Forum, has been at the center of global affairs for over four decades. He is convinced that the period of change we are living through is more significant, and the ramifications of the latest technological revolution more profound than any prior period of human history. He has dubbed this era the fourth industrial revolution. Crowdsourcing ideas, insights and wisdom from the World Economic Forum's global network of business, government, civil society and youth leaders, this book looks deeply at the future that is unfolding today and how we might take collective responsibility to ensure it is a positive one for all of us. The fourth industrial revolution. Historical context -- Profound and systematic change -- Drivers. Megatrends -- Tipping points -- Impact. Economy -- Business -- National and global -- Society -- The individual -- The way forward -- Appendix: Deep Shift. Implantable technologies -- Our digital presence -- Vision as the new interface -- Wearable Internet -- Ubiquitous computing -- A supercomputer in your pocket -- Storage for all -- The Internet of and for things -- The connected home -- Smart cities -- Big data for decisions -- Driverless cars -- Artificial intelligence and decision-making -- AI and white-collar jobs -- Robotics and services -- Bitcoin and the blockchain -- The sharing economy -- Governments and the blockchain -- 3D Printing and manufacturing -- 3D Printing and human health -- 3D Printing and consumer products -- Designer beings -- Neurotechnologies.},
	Author = {Schwab, Klaus and {World Economic Forum.}},
	Pages = {184},
	Publisher = {World Economic Forum},
	Title = {{The fourth industrial revolution}},
	Year = {2016}}

@book{Autonomous2005,
	Author = {Bekey, George A.},
	Publisher = {MIT Press},
	Title = {{Autonomous robots : from biological inspiration to implementation and control}},
	Year = {2005}}

@book{ProbRobot2005,
	Abstract = {Probablistic robotics is a growing area in the subject, concerned with perception and control in the face of uncertainty and giving robots a level of robustness in real-world situations. This book introduces techniques and algorithms in the field. Recursive state estimation -- Gaussian filters -- Robot motion -- Robot perception -- Mobile robot localization : Markov and Gaussian -- Mobile robot localization : grid and Monte Carlo -- Occupancy grid mapping -- Simultaneous localization and mapping -- The graphSLAM algorithm -- The sparse extended information filter -- The fastSLAM algorithm -- Markov decision processes -- Partially observable Markov decision processes -- Approximate POMDP techniques -- Exploration.},
	Author = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
	Pages = {647},
	Publisher = {MIT Press},
	Title = {{Probabilistic robotics}},
	Year = {2005}}

@article{Ballard1991,
	Abstract = {Animate vision systems have gaze control mechanisms that can actively position the camera coordinate system in response to physical stimuli. Compared to passive systems, animate systems show that visual computation can be vastly less expensive when considered in the larger context of behavior. The most important visual behavior is the ability to control the direction of gaze. This allows the use of very low resolution imaging that has a high virtual resolution. Using such a system in a controlled way provides additional constraints that dramatically simplify the computations of early vision. Another important behavior is the way the environment "behaves". Animate systems under real-time constraints can further reduce their computational burden by using environmental cues that are perspicuous in the local context. A third source of economy is introduced when behaviors are learned. Because errors are rarely fatal, systems using learning algorithms can amortize computational cost over extended periods. Further economies can be achieved when the learning system uses indexical reference, which is a form of dynamic variable binding. Animate vision is a natural way of implementing this dynamic binding. {\textcopyright} 1991.},
	Author = {Ballard, Dana H.},
	File = {:Users/tmats/papers/1-s2.0-0004370291900804-main.pdf:pdf},
	Journal = {Artificial Intelligence},
	Number = {1},
	Pages = {57--86},
	Title = {{Animate vision}},
	Volume = {48},
	Year = {1991}}

@article{Aloimonos1988,
	Abstract = {We investigate several basic problems in vision under the assumption$\backslash$nthat the observer is active. An observer is called active when engaged$\backslash$nin some kind of activity whose purpose is to control the geometric$\backslash$nparameters of the sensory apparatus. The purpose of the activity$\backslash$nis to manipulate the constraints underlying the observed phenomena$\backslash$nin order to improve the quality of the perceptual results. For example$\backslash$na monocular observer that moves with a known or unknown motion or$\backslash$na binocular observer that can rotate his eyes and track environmental$\backslash$nobjects are just two examples of an observer that we call active.$\backslash$nWe prove that an active observer can solve basic vision problems$\backslash$nin a much more efficient way than a passive one. Problems that are$\backslash$nill-posed and nonlinear for a passive observer become well-posed$\backslash$nand linear for an active observer. In particular, the problems of$\backslash$nshape from shading and depth computation, shape from contour, shape$\backslash$nfrom texture, and structure from motion are shown to be much easier$\backslash$nfor an active observer than for a passive one. It has to be emphasized$\backslash$nthat correspondence is not used in our approach, i.e., active vision$\backslash$nis not correspondence of features from multiple viewpoints. Finally,$\backslash$nactive vision here does not mean active sensing, and this paper introduces$\backslash$na general methodology, a general framework in which we believe low-level$\backslash$nvision problems should be addressed.},
	Author = {Aloimonos, J and Weiss, I and Bandyopadhyay, A},
	File = {:Users/tmats/papers/BF00133571.pdf:pdf},
	Journal = {International Journal of Computer Vision},
	Number = {4},
	Pages = {333--356},
	Title = {{Active vision}},
	Volume = {1},
	Year = {1988}}

@inproceedings{Vincent2008,
	Abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.},
	Author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	Booktitle = {Proceedings of the 25th international conference on Machine learning},
	File = {:Users/tmats/papers/592.pdf:pdf},
	Pages = {1096--1103},
	Title = {{Extracting and composing robust features with denoising autoencoders}},
	Year = {2008}}

@article{Doersch2016,
	Abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
	Author = {Doersch, Carl},
	File = {:Users/tmats/papers/1606.05908.pdf:pdf},
	Keywords = {neural networks,prediction,structured,unsupervised learning,variational autoencoders},
	Title = {{Tutorial on Variational Autoencoders}},
	Year = {2016}}

@inproceedings{Goodfellow2014,
	Abstract = {We propose a new framework for estimating generative models via an adversar- ial process; in which we simultaneously train two models: a generative model G that captures the data distribution; and a discriminative model D that estimates the probability that a sample came from the training data rather thanG. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D; a unique solution exists; with G recovering the training data distribution andD equal to 1 2 everywhere. In the case where G andD are defined by multilayer perceptrons; the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. 1},
	Author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	Booktitle = {Advances in Neural Information Processing Systems 27},
	File = {:Users/tmats/papers/5423-generative-adversarial-nets.pdf:pdf},
	Pages = {2672--2680},
	Title = {{Generative Adversarial Nets}},
	Year = {2014}}

@inproceedings{vae,
	Abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	Author = {Kingma, Diederik P and Welling, Max},
	Booktitle = {Proceedings of the 2nd International Conference on Learning Representations (ICLR)},
	Date-Modified = {2019-01-09 01:09:31 +0900},
	File = {:Users/tmats/papers/1312.6114.pdf:pdf},
	Title = {{Auto-Encoding Variational Bayes}},
	Year = {2013}}

@inproceedings{Schmidhuber1991,
	Author = {Meyer, Jean Arcady and Wilson, Stewart W.},
	Booktitle = {Proceedings of the International Conference on Simulation of Adaptive Behavior},
	File = {:Users/tmats/papers/6294131.pdf:pdf},
	Pages = {222--227},
	Title = {{A possibility for implementing curiosity and boredom in model-building neural controllers}},
	Year = {1991}}

@article{Cheng2016,
	Abstract = {In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.},
	Author = {Cheng, Jianpeng and Dong, Li and Lapata, Mirella},
	File = {:Users/tmats/papers/1601.06733.pdf:pdf},
	Title = {{Long Short-Term Memory-Networks for Machine Reading}},
	Year = {2016}}

@book{Aloimonos1993,
	Abstract = {Introduction: Active Vision Revisited / Yiannis Aloimonos -- 1. Active Vision as a Methodology / Kourosh Pahlavan, Tomas Uhlin and Jan-Olof Eklundh -- 2. Designing Visual Systems: Purposive Navigation / Yiannis Aloimonos, Ehud Rivlin and Liuqing Huang -- 3. Navigational Preliminaries / Cornelia Fermuller -- 4. Vision During Action / Giulio Sandini, Francesca Gandolfo, Enrico Grosso and Massimo Tistarelli -- 5. Visual Servoing from 2-D Image Cues / Daniel Raviv and Martin Herman -- 6. Computational Modelling of Hand-Eye Coordination / Andrew Blake -- 7. Principles of Animate Vision / Dana H. Ballard and Christopher M. Brown.},
	Author = {Aloimonos, John.},
	Pages = {292},
	Publisher = {Lawrence Erlbaum Associates, Publishers},
	Title = {{Active perception}},
	Year = {1993}}

@book{SuttonBook1998,
	Abstract = {Presents the book "Reinforcement Learning: An Introduction," written by Richard S. Sutton and Andrew G. Barto and published by the Massachusetts Institute of Technology (MIT) Press in 1998. The book is a textbook targeted toward engineers and scientists in artificial intelligence, operations research, neural networks, and control systems. Examines a computation approach to learning from interaction with environment. 1. Introduction -- 2. Evaluative feedback -- 3. The reinforcement learning problem -- 4. Dynamic programming -- 5. Monte carlo methods -- 6. Temporal-difference learning -- 7. Eligibility traces -- 8. Generalization and function approximation -- 9. Planning and learning -- 10. Dimensions of reinforcement learning -- 11. Case studies.},
	Author = {Sutton, Richard S. and Barto, Andrew G.},
	Pages = {322},
	Publisher = {MIT Press},
	Title = {{Reinforcement learning : an introduction}},
	Year = {1998}}

@book{DLBook2016,
	Abstract = {"Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and video games. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors"--Page 4 of cover. Introduction -- APPLIED MATH AND MACHINE LEARNING BASICS -- Linear algebra -- Probability and information theory -- Numerical computation -- Machine learning basics -- DEEP NETWORKS: MODERN PRACTICES -- Deep feedforward networks -- Regularization for deep learning -- Optimization for training deep models -- Convolutional networks -- Sequence modeling: recurrent and recursive nets -- Practical methodology -- Applications -- DEEP LEARNING RESEARCH -- Linear factor models -- Autoencoders -- Representation learning -- Structured probabilistic models for deep learning -- Monte Carlo methods -- Confronting the partition function -- Approximate inference -- Deep generative models.},
	Author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	Pages = {775},
	Publisher = {MIT Press},
	Title = {{Deep learning}},
	Year = {2016}}

@inproceedings{Larochelle2010,
	Abstract = {We describe a model based on a Boltzmann machine with third - order connections that can learn how to accumulate information about a shape over several fixations. The model uses a retina that only has enough high resolution pixels to cover a small area of the image, so it must ...},
	Author = {Larochelle, Hugo and Hinton, Geoffrey},
	Booktitle = {Advances in Neural Information Processing Systems 23},
	File = {:Users/tmats/papers/4089-learning-to-combine-foveal-glimpses-with-a-third-order-boltzmann-machine.pdf:pdf},
	Pages = {1243--1251},
	Title = {{Learning to combine foveal glimpses with a third-order Boltzmann machine}},
	Year = {2010}}

@article{Matsubara2014,
	Author = {松原仁},
	File = {:Users/tmats/papers/KJ00009359909.pdf:pdf},
	Journal = {人工知能},
	Keywords = {artificial intelligence,astro boy,frame problem,symbol grounding problem},
	Number = {3},
	Pages = {263--264},
	Title = {汎用人工知能への期待},
	Volume = {29},
	Year = {2014}}

@book{AI2017,
	Author = {AI白書編集委員会, 独立行政法人情報処理推進機構},
	Publisher = {角川アスキー総合研究所},
	Title = {{AI白書}},
	Year = {2017}}

@article{2010,
	Author = {神嶌敏弘},
	File = {:Users/tmats/papers/2010-s-jsai{\_}tl.pdf:pdf},
	Journal = {人工知能},
	Number = {4},
	Title = {転移学習},
	Volume = {25},
	Year = {2010}}

@article{Denil2011,
	Abstract = {We discuss an attentional model for simultaneous object tracking and recognition that is driven by gaze data. Motivated by theories of perception, the model consists of two interacting pathways: identity and control, intended to mirror the what and where pathways in neuroscience models. The identity pathway models object appearance and performs classification using deep (factored)-Restricted Boltzmann Machines. At each point in time the observations consist of foveated images, with decaying resolution toward the periphery of the gaze. The control pathway models the location, orientation, scale and speed of the attended object. The posterior distribution of these states is estimated with particle filtering. Deeper in the control pathway, we encounter an attentional mechanism that learns to select gazes so as to minimize tracking uncertainty. Unlike in our previous work, we introduce gaze selection strategies which operate in the presence of partial information and on a continuous action space. We show that a straightforward extension of the existing approach to the partial information setting results in poor performance, and we propose an alternative method based on modeling the reward surface as a Gaussian Process. This approach gives good performance in the presence of partial information and allows us to expand the action space from a small, discrete set of fixation points to a continuous domain.},
	Author = {Denil, Misha and Bazzani, Loris and Larochelle, Hugo and de Freitas, Nando},
	File = {:Users/tmats/papers/1109.3737.pdf:pdf},
	Journal = {Neural Computation},
	Keywords = {atten-,bandits,bayesian optimization,deep learning,particle filtering,restricted boltzmann machines,saliency,tion},
	Number = {8},
	Pages = {2151--2184},
	Title = {{Learning where to Attend with Deep Architectures for Image Tracking}},
	Volume = {24},
	Year = {2011}}

@book{Korekara2016,
	Abstract = {文献あり 索引あり.},
	Author = {牧野貴樹 and 澁谷長史 and 白川真一 and 浅田稔 and 麻生英樹 and 荒井幸代},
	Publisher = {森北出版},
	Title = {これからの強化学習},
	Year = {2016}}

@article{Friston2012,
	Abstract = {Recent years have seen the emergence of an important new fundamental theory of brain function. This theory brings information-theoretic, Bayesian, neuroscientific, and machine learning approaches into a single framework whose overarching principle is the minimiza- tion of surprise (or, equivalently, the maximization of expectation).The most comprehensive such treatment is the ``free-energy minimization'' formulation due to Karl Friston (see e.g., Friston and Stephan, 2007; Friston, 2010a,b -- see also Fiorillo, 2010; Thornton, 2010). A recurrent puzzle raised by critics of these models is that biological systems do not seem to avoid surprises. We do not simply seek a dark, unchanging chamber, and stay there. This is the ``Dark-Room Problem.'' Here, we describe the problem and further unpack the issues to which it speaks. Using the same format as the prolog of Eddington's Space, Time, and Gravitation (Eddington, 1920) we present our discussion as a conversation between: an information theorist (Thornton), a physicist (Friston), and a philosopher (Clark).},
	Author = {Friston, Karl and Thornton, Christopher and Clark, Andy},
	File = {:Users/tmats/papers/fpsyg-03-00130.pdf:pdf},
	Journal = {Frontiers in Psychology},
	Keywords = {Bayesian brain,Free-energy principle,Optimality,Surprise},
	Pages = {130},
	Pmid = {22586414},
	Title = {{Free-energy minimization and the dark-room problem}},
	Volume = {3},
	Year = {2012}}

@inproceedings{Krizhevsky2012,
	Abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
	Author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	Booktitle = {Advances in Neural Information Processing Systems 25},
	File = {:Users/tmats/papers/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf:pdf},
	Pages = {3642--3649},
	Title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
	Year = {2012}}

@article{Mnih2015,
	Abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6--8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9--11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
	Author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	Journal = {Nature},
	Number = {7540},
	Pages = {529--533},
	Title = {{Human-level control through deep reinforcement learning}},
	Volume = {518},
	Year = {2015}}

@article{Rensink2000,
	Abstract = {One of the more compelling impressions provided by vision is that of a coherent, richly-detailed world where everything is present simultaneously. Indeed, this impression is so compelling that we tend to ascribe these properties not only to the external world, but to our internal representations as well. But results from several recent experiments argue against this latter ascription. For example, changes in images of real-world scenes often go unnoticed when made during a saccade, flicker, blink, or movie cut. This "change blindness" provides strong evidence against the idea that our brains contain a picture-like representation of the scene that is everywhere detailed and coherent. How then do we represent a scene? It is argued here that focused attention provides spatiotemporal coherence for the stable representation of one object at a time. It is then argued that the allocation of attention can be coordinated to create a "virtual representation". In such a scheme, a stable object representation is formed whenever needed, making it appear to higher levels as if all objects in the scene are represented in detail simultaneously.},
	Author = {Rensink, Ronald A.},
	File = {:Users/tmats/papers/2018{\_}1{\_}7{\_}The Dynami.pdf:pdf},
	Journal = {Visual Cognition},
	Number = {1-3},
	Pages = {17--42},
	Title = {{The Dynamic Representation of Scenes}},
	Volume = {7},
	Year = {2000}}

@article{Solomonoff1956,
	Author = {Solomonoff, Grace},
	File = {:Users/tmats/papers/dartray.pdf:pdf},
	Pages = {1--28},
	Title = {{Ray Solomonoff and the Dartmouth Summer Research Project in Artificial Intelligence , 1956 Introduction : The Mysterious Science}},
	Year = {1956}}

@inproceedings{Ciresan2012,
	Abstract = {Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.},
	Author = {Cire{\c s}an, Dan and Meier, Ueli and Schmidhuber, Juergen},
	Booktitle = {Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	File = {:Users/tmats/papers/1202.2745.pdf:pdf},
	Pages = {3642--3649},
	Title = {{Multi-column Deep Neural Networks for Image Classification}},
	Year = {2012}}

@article{Ostrovski2017,
	Abstract = {Bellemare et al. (2016) introduced the notion of a pseudo-count, derived from a density model, to generalize count-based exploration to non-tabular reinforcement learning. This pseudo-count was used to generate an exploration bonus for a DQN agent and combined with a mixed Monte Carlo update was sufficient to achieve state of the art on the Atari 2600 game Montezuma's Revenge. We consider two questions left open by their work: First, how important is the quality of the density model for exploration? Second, what role does the Monte Carlo update play in exploration? We answer the first question by demonstrating the use of PixelCNN, an advanced neural density model for images, to supply a pseudo-count. In particular, we examine the intrinsic difficulties in adapting Bellemare et al.'s approach when assumptions about the model are violated. The result is a more practical and general algorithm requiring no special apparatus. We combine PixelCNN pseudo-counts with different agent architectures to dramatically improve the state of the art on several hard Atari games. One surprising finding is that the mixed Monte Carlo update is a powerful facilitator of exploration in the sparsest of settings, including Montezuma's Revenge.},
	Author = {Ostrovski, Georg and Bellemare, Marc G. and van den Oord, Aaron and Munos, Remi},
	File = {:Users/tmats/papers/1703.01310.pdf:pdf},
	Journal = {Advances in Neural Information Processing Systems},
	Title = {{Count-Based Exploration with Neural Density Models}},
	Volume = {29},
	Year = {2016}}

@article{Strehl2008,
	Abstract = {Several algorithms for learning near-optimal policies in Markov Decision Processes have been analyzed and proven efficient. Empirical results have suggested that Model-based Interval Estimation (MBIE) learns efficiently in practice, effectively balancing exploration and exploitation. This paper presents a theoretical analysis of MBIE and a new variation called MBIE-EB, proving their efficiency even under worst-case conditions. The paper also introduces a new performance metric, average loss, and relates it to its less "online" cousins from the literature. ?? 2008 Elsevier Inc. All rights reserved.},
	Author = {Strehl, Alexander L. and Littman, Michael L.},
	Doi = {10.1016/j.jcss.2007.08.009},
	File = {:Users/tmats/papers/1-s2.0-S0022000008000767-main.pdf:pdf},
	Issn = {00220000},
	Journal = {Journal of Computer and System Sciences},
	Keywords = {Learning theory,Markov Decision Processes,Reinforcement learning},
	Number = {8},
	Pages = {1309--1331},
	Publisher = {Elsevier Inc.},
	Title = {{An analysis of model-based Interval Estimation for Markov Decision Processes}},
	Url = {http://dx.doi.org/10.1016/j.jcss.2007.08.009},
	Volume = {74},
	Year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.jcss.2007.08.009}}

@inproceedings{Bellemare2016,
	Abstract = {We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across observations. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into intrinsic rewards and obtain significantly improved exploration in a number of hard games, including the infamously difficult Montezuma's Revenge.},
	Author = {Bellemare, Marc G. and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
	Booktitle = {Advances in Neural Information Processing Systems 29},
	File = {:Users/tmats/papers/1606.01868.pdf:pdf},
	Pages = {1471--1479},
	Title = {{Unifying Count-Based Exploration and Intrinsic Motivation}},
	Year = {2016}}

@article{Willia1992,
	Abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
	Author = {Willia, Ronald J.},
	File = {:Users/tmats/papers/10.1007{\%}2FBF00992696.pdf:pdf},
	Journal = {Machine Learning},
	Keywords = {Reinforcement learning,connectionist networks,gradient descent,mathematical analysis},
	Number = {3},
	Pages = {229--256},
	Title = {{Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning}},
	Volume = {8},
	Year = {1992}}

@article{Poupart2006,
	Abstract = {Reinforcement learning (RL) was originally proposed as a framework to allow agents to learn in an online fashion as they interact with their environment. Existing RL algorithms come short of achieving this goal because the amount of exploration required is often too costly and/or too time consuming for online learning. As a result, RL is mostly used for offline learning in simulated environments. We propose a new algorithm, called BEETLE, for effective online learning that is computationally efficient while minimizing the amount of exploration. We take a Bayesian model-based approach, framing RL as a partially observable Markov decision process. Our two main contributions are the analytical derivation that the optimal value function is the upper envelope of a set of multivariate polynomials, and an efficient point-based value iteration algorithm that exploits this simple parameterization.},
	Author = {Poupart, Pascal and Vlassis, Nikos and Hoey, Jesse and Regan, Kevin},
	Doi = {10.1145/1143844.1143932},
	File = {:Users/tmats/papers/p697-poupart.pdf:pdf},
	Isbn = {1595933832},
	Issn = {{\textless}null{\textgreater}},
	Journal = {Proceedings of the 23rd international conference on Machine learning - ICML '06},
	Pages = {697--704},
	Title = {{An analytic solution to discrete Bayesian reinforcement learning}},
	Url = {http://portal.acm.org/citation.cfm?doid=1143844.1143932},
	Year = {2006},
	Bdsk-Url-1 = {http://portal.acm.org/citation.cfm?doid=1143844.1143932},
	Bdsk-Url-2 = {https://doi.org/10.1145/1143844.1143932}}

@article{Seth2014,
	Abstract = {Is there a single principle by which neural operations can account for perception, cognition, action, and even consciousness? A strong candidate is now taking shape in the form of ``predictive processing''. On this theory, brains engage in predictive inference on the causes of sensory inputs by continuous minimization of prediction errors or informational ``free energy''. Predictive processing can account, supposedly, not only for perception, but also for action and for the essential contribution of the body and environment in structuring sensorimotor interactions. In this paper I draw together some recent developments within predictive processing that involve predictive modelling of internal physiological states ( interoceptive inference ), and integration with ``enactive'' and ``embodied'' approaches to cognitive science ( predictive perception of sensorimotor contingencies ). The upshot is a development of predictive processing that originates, not in Helmholtzian perception-as-inference, but rather in 20 th -century cybernetic principles that emphasized homeostasis and predictive control. This way of thinking leads to (i) a new view of emotion as active interoceptive inference; (ii) a common predictive framework linking experiences of body ownership, emotion, and exteroceptive perception; (iii) distinct interpretations of active inference as involving disruptive and disambiguatory ---not just confirmatory ---actions to test perceptual hypotheses; (iv) a neurocognitive operationalization of the ``mastery of sensorimotor contingencies'' (where sensorimotor contingencies reflect the rules governing sensory changes produced by various actions); and (v) an account of the sense of subjective reality of perceptual contents (``perceptual presence'') in terms of the extent to which predictive models encode potential sensorimotor relations (this being ``counterfactual richness''). This is rich and varied territory, and surveying its landmarks emphasizes the need for experimental tests of its key contributions.},
	Author = {Seth, AK},
	File = {:Users/tmats/papers/The Cybernetic Bayesian Brain.pdf:pdf},
	Journal = {Open MIND},
	Keywords = {active inference,botics,counterfactually-equipped predictive model,evolutionary ro-,free energy principle,interoception,perceptual presence,predictive,processing,sensorimotor contingencies,somatic marker hypothesis,synaes-},
	Pages = {1--24},
	Title = {{The cybernetic bayesian brain - from interoceptive inference to sensorimotor contingencies}},
	Volume = {35},
	Year = {2014}}

@article{Bajcsy2017,
	Abstract = {Despite the recent successes in robotics, artificial intelligence and computer vision, a complete artificial agent necessarily must include active perception. A multitude of ideas and methods for how to accomplish this have already appeared in the past, their broader utility perhaps impeded by insufficient computational power or costly hardware. The history of these ideas, perhaps selective due to our perspectives, is presented with the goal of organizing the past literature and highlighting the seminal contributions. We argue that those contributions are as relevant today as they were decades ago and, with the state of modern computational tools, are poised to find new life in the robotic perception systems of the next decade.},
	Author = {Bajcsy, Ruzena and Aloimonos, Yiannis and Tsotsos, John K.},
	File = {:Users/tmats/papers/1603.02729.pdf:pdf},
	Journal = {Autonomous Robots},
	Keywords = {Attention,Control,Perception,Sensing},
	Title = {{Revisiting active perception}},
	Year = {2017}}

@article{Land2006,
	Abstract = {The patterns of eye movement that accompany static activities such as reading have been studied since the early 1900s, but it is only since head-mounted eye trackers became available in the 1980s that it has been possible to study active tasks such as walking, driving, playing ball games and ordinary everyday activities like food preparation. This review examines the ways that vision contributes to the organization of such activities, and in particular how eye movements are used to locate the information needed by the motor system in the execution of each act. Major conclusions are that the eyes are proactive, typically seeking out the information required in the second before each act commences, although occasional 'look ahead' fixations are made to establish the locations of objects for use further into the future. Gaze often moves on before the last act is complete, indicating the presence of an information buffer. Each task has a characteristic but flexible pattern of eye movements that accompanies it, and this pattern is similar between individuals. The eyes rarely visit objects that are irrelevant to the action, and the conspicuity of objects (in terms of low-level image statistics) is much less important than their role in the task. Gaze control may involve movements of eyes, head and trunk, and these are coordinated in a way that allows for both flexibility of movement and stability of gaze. During the learning of a new activity, the eyes first provide feedback on the motor performance, but as this is perfected they provide feed-forward direction, seeking out the next object to be acted upon. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
	Author = {Land, Michael F.},
	File = {:Users/tmats/papers/1-s2.0-S1350946206000036-main.pdf:pdf},
	Journal = {Progress in Retinal and Eye Research},
	Number = {3},
	Pages = {296--324},
	Title = {{Eye movements and the control of actions in everyday life}},
	Volume = {25},
	Year = {2006}}

@article{Regan2001,
	Abstract = {Many current neurophysiological, psychophysical, and psychological approaches to vision rest on the idea that when we see, the brain produces an internal representation of the world. The activation of this internal representation is assumed to give rise to the experience of seeing. The problem with this kind of approach is that it leaves unexplained how the existence of such a detailed internal representation might produce visual consciousness. An alternative proposal is made here. We propose that seeing is a way of acting. It is a particular way of exploring the environment. Activity in internal representations does not generate the experience of seeing. The out- side world serves as its own, external, representation. The experience of seeing occurs when the organism masters what we call the gov- erning laws of sensorimotor contingency. The advantage of this approach is that it provides a natural and principled way of accounting for visual consciousness, and for the differences in the perceived quality of sensory experience in the different sensory modalities. Sev- eral lines of empirical evidence are brought forward in support of the theory, in particular: evidence from experiments in sensorimotor adaptation, visual ``filling in,'' visual stability despite eye movements, change blindness, sensory substitution, and color perception.},
	Author = {O'Regan, J. Kevin and No{\"{e}}, Alva},
	File = {:Users/tmats/papers/ORegan.pdf:pdf},
	Journal = {Behavioral and Brain Sciences},
	Number = {05},
	Pages = {939--973},
	Title = {{A sensorimotor account of vision and visual consciousness}},
	Volume = {24},
	Year = {2001}}

@book{Kandel2013,
	Abstract = {5th ed. Machine generated contents note: PART I: Overall Perspective (Kandel, Hudspeth)1. The Brain and Behavior (Kandel, Hudspeth)2. Nerve Cells, Neural Circuitry, and Behavior (Kandel, Barres, Hudspeth)3. Genes and Behavior (Bargmann, Gilliam) PART II: Cell and Molecular Biology of the Neuron (Siegelbaum, Hudspeth)4. The Cells of the Nervous System (Schwartz, Barres, Goldman)5. Ion Channels (Siegelbaum, Koester)6. Membrane Potential and the Passive Electrical Properties of the Neuron (Siegelbaum, Koester)7. Propagated Signaling: The Action Potential (Siegelbaum, Koester) PART III: Overview: Synaptic Transmission (Siegelbaum, Hudspeth)8. Overview of Synaptic Transmission (Siegelbaum, Kandel)9Signaling at the Nerve Muscle Synapse: Directly Gated Transmission (Siegelbaum, Kandel)10. Synaptic Integration in the Central Nervous System (Siegelbaum, Kandel, Yuste)11. Modulation of Synaptic Transmission: Second Messengers (Clapham, Siegelbaum, Schwartz)12. Transmitter Release (Siegelbaum, Kandel, Sudhof)13. Neurotransmitters (Schwartz, Javitch)14. Diseases of Nerve and the Motor Unit (Brown, Cannon, Rowland) PART IV: The Neural Basis of Cognition (Hudspeth, Kandel)15. The Organization of the Central Nervous System (Amaral, Strick)16. The Functional Organization of Perception and Movement (Amaral)17. From Nerve Cells to Cognition: The Internal Representations for Space and Action (Kandel)18. The Organization of Cognition (Olson, Colby)19. Cognitive Functions of the Premotor System (Rizzolatti, Strick)20. Functional Imaging of Cognition (Small, Heeger) PART V: Perception (Hudspeth)21. Sensory Coding (Gardner, Johnson)22. The Somatosensory System: Receptors and Central Pathways (Gardner, Johnson)23. Touch (Gardner, Johnson)24. Pain (Basbaum, Jessell)25. The Constructive Nature of Visual Processing (Gilbert)26. Low-Level Visual Processing: The Retina (Meister, Tessier-Lavigne)27. Intermediate-Level Visual Processing: Visual Primitives (Gilbert)28. High-Level Visual Processing: Cognitive Influences (Albright)29. Visual Processing and Action (Wurtz, Goldberg)30. The Inner Ear (Hudspeth)31. The Auditory Central Nervous System (Oertel, Doupe)32. Smell and Taste: The Chemical Senses (Buck, Bargmann)PART VI: Movement (Hudspeth)33. The Organization and Planning of Movement (Wolpert, Pearson, Ghez)34. The Motor Unit and Muscle Action (Enoka, Pearson)35. Spinal Reflexes (Pearson, Gordon)36. Locomotion (Pearson, Gordon)37. Voluntary Movement: The Primary Motor Cortex (Kalaska, Rizzolatti)38. Voluntary Movement: The Parietal and Premotor Cortex (Rizzolatti, Kalaska)39. The Control of Gaze(Goldberg)40. The Vestibular System (Goldberg, Hudspeth)41. Posture (MacPherson, Horack)42. The Cerebellum (Lisberger, Thach)43. The Basal Ganglia (Wichmann, DeLong)44. Genetic Mechanisms in Degenerative Diseases of the Nervous System (Zoghbi)PART VII: The Unconscious and Conscious Processing of Neural Information (Kandel, Siegelbaum, Schwartz)45. The Sensory, Motor, and Reflex Functions of the Brain Stem (Saper, Lumsden, Richerson) 46. The Modulatory Functions of the Brain Stem (Richerson, Aston-Jones, Saper)47. The Autonomic Motor System and the Hypothalamus (Horn, Swanson)48. Emotions and Feelings (LeDoux, Damasio)49. Homeostasis, Motivation, and Addictive States (Shizgal, Hyman)50. Seizures and Epilepsy (Westbrook)51. Sleep and Dreaming (McCormick, Westbrook)PART VIII: Development and the Emergence of Behavior (Jessell)52. Patterning the Nervous System (Jessell, Sanes)53. Differentiation and Survival of Nerve Cells(Jessell, Sanes)54. The Growth and Guidance of Axons (Sanes, Jessell)55. Formation and Elimination of Synapses (Sanes, Jessell)56. Experience and the Refinement of Synaptic Connections (Jessell, Sanes)57. Repairing the Damaged Brain (Sanes, Jessell)58. Sexual Differentiation of the Nervous System (Shah, Jessell, Sanes)59. The Aging Brain (Jessell, Sanes)PART IX: Language, Thought, Affect, and Learning (Kandel, Schwartz)60. Language (Kuhl, Damasio)61. Disorders of Conscious and Unconscious Mental Processes (C. Frith)62. Disorders of Thought and Volition: Schizophrenia (Hyman, Cohen)63. Disorders of Mood and Anxiety (Hyman, Cohen)64. Autism and Other Neurodevelopmental Disorders Affecting Cognition (U. Frith, Happe, Amaral, and Warren)65. Learning and Memory(Schacter, Wagner)66. Cellular Mechanisms of Implicit Memory Storage and the Biological Basis of Individuality (Kandel, Siegelbaum)67. Prefrontal Cortex, Hippocampus, and the Biology of Explicit Memory Storage)AppendicesA. Review of Basic Circuit Theory (Koester)B. The Neurological Examination of the Patient (Kriegstein, Brust)C. Circulation of the Brain (Brust)D. The Blood-Brain Barrier, Choroid Plexus, and Cerebrospinal Fluid (Laterra, Goldstein)E. Neural Networks (Seung, Yuste)F. Theoretical Approaches to Neuroscience: Examples From Single Neurons to Networks (Abbott, Fusi, Miller). "The field's definitive work from a Nobel Prize-winning author 900 full-color illustrations Principles of Neural Science, 5e describes our current understanding of how the nerves, brain, and mind function. From molecules to anatomic structures and systems to cognitive function, this comprehensive reference covers all aspects of neuroscience. Widely regarded as the field's cornerstone reference, the fifth edition is highlighted by more than 900 full-color illustrations. The fifth edition has been completely updated to reflect the tremendous amount of new research and development in neuroscience in the last decade. Lead author Eric Kandel was awarded the Nobel Prize in Physiology or Medicine in 2000"--Provided by publisher. The brain and behavior -- Nerve cells, neural circuitry, and behavior -- Genes and behavior -- The cells of the nervous system -- Ion Channels -- Membrane potential and the passive electrical properties of the neuron -- Propagated signaling: the action potential -- Overview of synaptic transmission -- Signaling at the nerve-muscle synapse: directly gated transmission -- Synaptic integration in the central nervous system -- Modulation of synaptic transmission: second messengers -- Transmitter release -- Neurotransmitters -- Diseases of the nerve and motor unit -- The organization of the central nervous system -- The functional organization of perception and movement -- From nerve cells to cognition: the internal representations of space and action -- The organization of cognition -- Cognitive functions of the premotor systems -- Functional imaging of cognition -- Sensory coding -- The somatosensory system: receptors and central pathways -- Touch -- Pain -- The constructive nature of visual processing -- Low-level visual processing: the retina -- Intermediate-level visual processing and visual primitives -- High-level visual processing: cognitive influences -- Visual processing and action -- The inner ear -- The auditory central nervous system -- Smell and taste: the chemical senses -- The organization and planning of movement -- The motor unit and muscle action -- Spinal reflexes -- Locomotion -- Voluntary movement: the primary motor cortex -- Voluntary movement: the parietal and premotor cortex -- The control of gaze -- The vestibular system -- Posture -- The cerebellum -- The basal ganglia -- Genetic mechanisms in degenerative diseases of the nervous system -- The sensory, motor, and reflex functions of the brain stem -- The modulatory functions of the brain stem -- The autonomic motor system and the hypothalamus -- Emotions and feelings -- Homeostasis, motivation, and addictive states -- Seizures and epilepsy -- Sleep and dreaming -- Patterning the nervous system -- Differentiation and survival of nerve cells -- The growth and guidance of axons -- Formation and elimination of synapses -- Experience and the refinement of synaptic connections -- Repairing the damaged brain -- Sexual differentiation of the nervous system -- The aging brain -- Language -- Disorders of conscious and unconscious mental processes -- Disorders of thought and volition: schizophrenia -- Disorders of mood and anxiety -- Autism and other neurodevelopmental disorders affecting cognition -- Learning and memory -- Cellular mechanisms of implicit memory storage and the biological basis of individuality -- Prefrontal cortex, hippocampus, and the biology of explicit memory storage -- Appendices: A. Review of basic circuit theory -- B. The neurological examination of the patient -- C. Circulation of the brain -- D. The blood-brain barrier, choroid plexus, and cerebrospinal fluid -- E. Neural networks -- F. Theoretical approaches to neuroscience: examples from single neurons to networks.},
	Author = {Kandel, Eric R.},
	Pages = {1709},
	Publisher = {McGraw-Hill Medical},
	Title = {{Principles of neural science}},
	Year = {2013}}

@article{Ba2014,
	Abstract = {We present an attention-based model for recognizing multiple objects in images. The proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image. We show that the model learns to both localize and recognize multiple objects despite being given only class labels during training. We evaluate the model on the challenging task of transcribing house number sequences from Google Street View images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation.},
	Author = {Ba, Jimmy and Mnih, Volodymyr and Kavukcuoglu, Koray},
	File = {:Users/tmats/papers/1412.7755.pdf:pdf},
	Pages = {1--10},
	Title = {{Multiple Object Recognition with Visual Attention}},
	Year = {2014},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxArLi4vLi4vLi4vLi4vRG93bmxvYWRzL1MxMzY0NjYxMzk5MDEyOTQyLmJpYk8RAWwAAAAAAWwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////xVTMTM2NDY2MTM5OTAxMjk0Mi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAQAAgAACiBjdQAAAAAAAAAAAAAAAAAJRG93bmxvYWRzAAACADIvOlVzZXJzOmlzaG9oZWkyMjA6RG93bmxvYWRzOlMxMzY0NjYxMzk5MDEyOTQyLmJpYgAOACwAFQBTADEAMwA2ADQANgA2ADEAMwA5ADkAMAAxADIAOQA0ADIALgBiAGkAYgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASADBVc2Vycy9pc2hvaGVpMjIwL0Rvd25sb2Fkcy9TMTM2NDY2MTM5OTAxMjk0Mi5iaWIAEwABLwAAFQACABH//wAAAAgADQAaACQAUgAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAHC}}

@inproceedings{Xu2015,
	Abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
	Author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
	Booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
	File = {:Users/tmats/papers/1502.03044.pdf:pdf},
	Pages = {2048--2057},
	Title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
	Year = {2015}}

@book{DLNLP2017,
	Author = {坪井祐太 and 海野裕也 and 鈴木潤},
	Publisher = {講談社},
	Series = {MLP機械学習プロフェッショナルシリーズ},
	Title = {深層学習による自然言語処理},
	Year = {2017}}

@inproceedings{Pathak2017,
	Abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
	Author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
	Booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	File = {:Users/tmats/papers/1705.05363.pdf:pdf},
	Pages = {2778--2787},
	Title = {{Curiosity-Driven Exploration by Self-Supervised Prediction}},
	Year = {2017},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAhLi4vLi4vLi4vLi4vRG93bmxvYWRzLzI5NDY3MDQuYmliTxEBRAAAAAABRAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////CzI5NDY3MDQuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAABAACAAAKIGN1AAAAAAAAAAAAAAAAAAlEb3dubG9hZHMAAAIAKC86VXNlcnM6aXNob2hlaTIyMDpEb3dubG9hZHM6Mjk0NjcwNC5iaWIADgAYAAsAMgA5ADQANgA3ADAANAAuAGIAaQBiAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAJlVzZXJzL2lzaG9oZWkyMjAvRG93bmxvYWRzLzI5NDY3MDQuYmliABMAAS8AABUAAgAR//8AAAAIAA0AGgAkAEgAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABkA==}}

@inproceedings{Vaswani2017,
	Abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	Author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	Booktitle = {Advances in Neural Information Processing Systems 30},
	File = {:Users/tmats/papers/7181-attention-is-all-you-need.pdf:pdf},
	Pages = {6000--6010},
	Title = {{Attention Is All You Need}},
	Year = {2017}}

@article{Sorokin2015,
	Abstract = {A deep learning approach to reinforcement learning led to a general learner able to train on visual input to play a variety of arcade games at the human and superhuman levels. Its creators at the Google DeepMind's team called the approach: Deep Q-Network (DQN). We present an extension of DQN by "soft" and "hard" attention mechanisms. Tests of the proposed Deep Attention Recurrent Q-Network (DARQN) algorithm on multiple Atari 2600 games show level of performance superior to that of DQN. Moreover, built-in attention mechanisms allow a direct online monitoring of the training process by highlighting the regions of the game screen the agent is focusing on when making decisions.},
	Archiveprefix = {arXiv},
	Arxivid = {1512.01693},
	Author = {Sorokin, Ivan and Seleznev, Alexey and Pavlov, Mikhail and Fedorov, Aleksandr and Ignateva, Anastasiia},
	Eprint = {1512.01693},
	File = {:Users/tmats/papers/1512.01693.pdf:pdf},
	Title = {{Deep Attention Recurrent Q-Network}},
	Year = {2015}}

@inproceedings{Mnih2014,
	Abstract = {Applying convolutional neural networks to large images is computationally ex- pensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is ca- pable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it per- forms can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.},
	Author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and Kavukcuoglu, Koray},
	Booktitle = {Advances in Neural Information Processing Systems 27},
	File = {:Users/tmats/Downloads/5542-recurrent-models-of-visual-attention.pdf:pdf},
	Pages = {2204--2212},
	Title = {{Recurrent Models of Visual Attention}},
	Year = {2014}}
