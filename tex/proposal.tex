\chapter{状態表現の階層性を考慮することによる深層状態空間モデルの拡張}
\label{chap:proposal}
を目指す．

はじめに，提案手法で扱う問題設定について整理し，その後提案手法の具体的な説明として，確率モデル・最適化とモデルアーキテクチャについて述べていく．

\section{問題設定}
本研究では行動条件付き映像予測の問題を解く．具体的には、ある行動主体が実行した行動系列$\vec{a}$とその結果観測される観測系列$\vec{o}$のセット$\{\vec{a}, \vec{o}\}$が予め多数与えられたときに、未知の初期状態と行動系列から結果として観測される映像を予測できるようにする．

\section{ベースライン}
本研究では2章で述べたシンプルなDSSMをベースラインにおく．

\section{提案手法}
3章で述べたように、ベースラインは潜在変数の次元を大きくすると学習がうまく行かない．提案手法は，より豊かな状態表現を獲得できるようにするようベースラインのDSSMを拡張することが目的である．

\subsection{状態表現の階層性}
まず、潜在変数の次元を変えた時に獲得される情報について考察する．状態表現学習によって，ベースラインモデルが潜在変数の次元を変えたときに獲得する情報と、潜在変数を高次元にした際に獲得してほしい情報、そしてすべての情報の関係性を考える．

\begin{figure}[tbp]
  \begin{center}
    \includegraphics[width=\linewidth]{./figures/hierarchical.png}
    \caption{hierarchical}
    \label{fig:hierarchical}
  \end{center}
\end{figure}


中心的な情報から順に獲得される．
中心的な情報が獲得できないうちに表面的な情報を獲得しても，中心的な表現が間違っていれば生成されるものも異なり，学習時に誤差を小さくできないからである．
というように、現実的なデータの背後にある状態表現には階層性がある．今、低次元であれば
まず背景、次によく動く部分、最後に細かいテクスチャ、という順番に学習されるのが合理的である．つまり、stateの次元を大きくしたときに学習される情報には図のような包含関係がある．

今、

理想的には、stateの次元を徐々に大きくすることである．
ベースラインでは、Fig \ref{fig:hierarchical}の赤で示した部分をいきなり学習しようとするが、


ただし状態表現の外側になるほど得られるデータが疎になるので，表現としては獲得しにくくなる
モデルに正規分布を仮定していることによる表現力の制約などを除けば、

\subsection{階層的な状態表現の遷移}
前節より、状態表現に階層性があることを述べたが，この節では階層的な状態表現の遷移を考える．
遷移にも順序がある．つまり，中心的な状態表現が遷移した後，
更新前を白で、更新後を灰色で表している
ベースラインのDSSMモデルは、提案するHDSSMの特別な場合と解釈することもできる．


\begin{figure}[tbp]
  \begin{center}
    \includegraphics[width=0.5\linewidth]{./figures/transition_base.png}
    \caption{transition base}
    \label{fig:transition_base}
  \end{center}
\end{figure}


\begin{figure}[tbp]
  \begin{center}
    \includegraphics[width=0.8\linewidth]{./figures/transition_proposal.png}
    \caption{transition proposal}
    \label{fig:transition_proposal}
  \end{center}
\end{figure}

% \caption[hoge]{fuga}
\begin{figure}[tbp]
  \begin{center}
    \includegraphics[width=\linewidth]{./figures/proposal.png}
    \caption{(縦に分割する！)点線の$s^1$, $s^2$の推論分布は簡単のため時刻t, t-1でのみ記載している．proposal (学習時) 2つずつ記載されている$o_t$は同じデータを示すが．異なるsから独立に生成されることを明示している．}
    \label{fig:proposal}
  \end{center}
\end{figure}


\subsection{確率モデル・最適化}

ここまでで、潜在変数の次元を大きくした際に，獲得できる状態表現とその遷移がどのようなものかを考察した．
考慮すべき点は以下のようになる．

1 stateに大きな次元を仮定した際，stateの推論が難しいため学習が難しく不安定になる． \\
2 状態表現には階層性があり，表面的な状態表現になるほど学習が難しくなるが、中心的な状態表現はベースライン手法で学習ができる． \\
3 状態表現の遷移には順序があリ，大きな潜在表現の遷移を直接学習するのは難しいが，中心的な潜在表現の遷移を学習することはベースライン手法でも可能である． \\

これらを踏まえて、次を満たすモデルを考えることにした．
1. a


最終的にFig. \ref{fig:proposal}のようなモデルを提案する．

$s^1$は$n^1$次元，$s^2$は$n^2$次元，$n^1 < n^2$である

確率的生成過程は以下
\begin{equation}
  p(o_{1:T}|a_{1:T}) = \prod_{t=1}^T \iint p(o_t|s^2_t) p(s^2_t|s^2_{t-1}, a_t, s^1_t) p(s^1_t|s^1_{t-1}, a_t) d{s^1_t}{ds^2_t}
\end{equation}


この時の変分下限は以下
\begin{eqnarray}
  \ (ELBO) \nonumber \\
  &=& \sum_{t=1}^T \left( \mathbb{E}_{s^2_t \sim q(s^2_t|a_{1:t}, o_{1:t})} [\log p(o_t|s^2_t)] \right. \nonumber \\
  && \hspace{2em} \left. - \mathbb{E}_{s^1_{t-1} \sim q(s^1_{t-1}|a_{1:t-1}, o_{1:t-1}), 改行したい s^2 \sim}  [\mathrm{D_{KL}}(q(s_t|s_{t-1}, a_t, o_t) \| p(s_t|s_{t-1}, a_t, o_t))] \right. \nonumber \\
  && \hspace{2em} \left. - \mathbb{E}_{s^1_{t-1} \sim q(s^1_{t-1}|a_{1:t-1}, o_{1:t-1}), 改行したい s^2 \sim } [\mathrm{D_{KL}}(q(s_t|s_{t-1}, a_t, o_t) \| p(s_t|s_{t-1}, a_t, o_t))] \right) \nonumber \\
  \label{eq:hssm_elbo}
\end{eqnarray}


目的関数は以下
% 
\begin{eqnarray}
  \ (目的関数) \nonumber \\
  &=& \sum_{t=1}^T \left( \mathbb{E}_{s^2_t \sim q(s^2_t|a_{1:t}, o_{1:t})} [\log p(o_t|s^2_t)] + \beta \mathbb{E}_{s^1_t \sim q(s^1_t|a_{1:t}, o_{1:t})} [\log p(o_t|s^1_t)] \right. \nonumber \\
  && \hspace{2em} \left. - \mathbb{E}_{s^1_{t-1} \sim q(s^1_{t-1}|a_{1:t-1}, o_{1:t-1}), 改行したい s^2 \sim}  [\mathrm{D_{KL}}(q(s_t|s_{t-1}, a_t, o_t) \| p(s_t|s_{t-1}, a_t, o_t))] \right. \nonumber \\
  && \hspace{2em} \left. - \mathbb{E}_{s^1_{t-1} \sim q(s^1_{t-1}|a_{1:t-1}, o_{1:t-1}), 改行したい s^2 \sim } [\mathrm{D_{KL}}(q(s_t|s_{t-1}, a_t, o_t) \| p(s_t|s_{t-1}, a_t, o_t))] \right) \nonumber \\
  \label{eq:hssm_loss}
\end{eqnarray}

ここで $\beta$ は学習中に更新されるハイパーパラメータで， $\beta = exp(-epoch*0.001)$としている．

提案モデルは，前節に従って状態表現の階層性とその遷移の順序を考慮したモデルで，また大きなstate$s^2_t$の推論に$s^1_t$を用いることで，stateに大きな次元を仮定した際にstateの遷移先の推論が難しいという問題を克服する． \\

3階層以上になった時も同様に，大きなstateは小さいstate全てを使って遷移する．
であり，階層を重ね，中心的な

$\beta$を導入しているのは，ある程度高次元のstateの遷移が安定したあとは
1良い低下一層の表現を得ることと，2再構成しやすい低次元の表現を得ること，は必ずしも一致しないため，本来の生成過程以外の

% ニューロンを殺していく


評価時には、$p(x|s^2)$を用いてデータを生成する．

\section{類似手法との差分}
DRAW[]はVAEの潜在変数に階層性をもたせたモデルで、データに遷移はないが、狙いは提案手法と似ている．多層RNN[]は直前の観測を逐次的に入力すplanetでRNNベースはダイナミクスの獲得が弱く，長期の予測が難しいと報告されている．FHMMは離散低次元でEMアルゴリズムによって最適化をしている

% videoflowはaction conditionalじゃないしー

