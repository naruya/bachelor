\chapter{序論}
\label{chap:introduction}
\section{本研究の背景}
\subsection{行動条件付き映像予測}
多様な環境で様々なタスクが遂行可能な汎用的なロボット（generalist robots）の開発はロボット工学の最重要課題の一つである．ロボットハードウェアの低価格化，汎用的なロボットソフトウェアの普及に加え，近年の急速な深層学習技術の発展を受けてロボットの制御方策を自ら学習させる{\bf ロボット学習}の研究が進んでおり，ロボットで遂行可能なタスクは着実に増えてきている．

ロボット学習において，将来予測，特に映像予測を明示的に学習することは，

\begin{itemize}
    \item ロボット自身が映像予測を用いた方策をたてることが可能になる
    \item 映像予測結果を人が評価することでロボットの行動を予め評価できる
\end{itemize}
という大きく二つの点からで重要であると言える．一点目のの映像予測を用いた方策の例として，Hafnerら\cite{hafner2019planet}は，強化学習の問題設定において明示的に学習した映像予測モデルを用いることで簡単なアルゴリズムで効率的なプランニングが可能であることを示した．二点目の映像予測を人が評価する例としてEbertらによる研究\cite{ebert2018visual}では学習した映像予測モデルを用いて，ロボットの操作によって予想される物体の移動の軌跡を確率分布として出力することができ，これを用いて人がロボットの行動の正しさを予め判断することができる．

このようにロボット学習における映像予測は重要であるが，映像予測だけを切り取って研究されることも多い．映像予測の中でも，ロボットの行動の結果として観測される映像を予測する問題設定を{\bf 行動条件付き映像予測}と呼び，様々な研究がなされてきている．近年高精度な行動条件付き映像予測手法がいくつか提案されており，ロボット学習研究で扱うタスクの高度化を背景にしてこれらの映像予測手法をより複雑な問題設定に対して適用していきたいと考えられているが，いくつかの研究で既存の行動条件付き映像予測は上手く機能しない可能性があることがわかってきている．ただしここでいう「より複雑な問題設定」とは具体的には環境中に複数の操作対象の物体が隣接し合って置いてある場合，操作対象が布などの非剛体物である場合などを想定している．次に既存の行動条件付き映像予測手法の問題点について示す．


\subsection{既存の行動条件付き映像予測手法の問題点}

行動条件付き映像予測手法は大きく{\bf 再帰型ニューラルネットワーク(RNN)ベースの手法}と{\bf 深層状態空間モデル(DSSM)ベースの手法}に分けられる．Hafnerら\cite{hafner2019planet}の研究など深層強化学習の問題で映像予測を明示的に行う場合は後者のDSSMベースの手法が多く採用されるが，映像予測の問題では前者のRNNベースの手法が多く使われている\cite{denton2018stochastic}\cite{villegas2019high}．

RNNベースの手法は予測した1ステップ先の画像を入力にして更に1ステップ先の画像を出力するというような，自らの出力を逐次入力する構造を持つ．RNNベースの手法はDSSMと比較して高精度な映像を生成に長けている反面，近年提案されているRNNベースの手法には以下のような問題点がある．

\begin{itemize}
    \item \textcolor{red}{「確率的な遷移を考慮できない」，は確率的な遷移+自己回帰な論文もあるので入れませんでした}
    \item 誤差が蓄積しやすい
    \item 文脈を必要とする
\end{itemize}

一点目について，RNNベースのモデルを用いると常に直前のフレームを参照して次のフレームを予測するため短い期間の予測であれば精度は高くなるが，予測誤差が蓄積していくために長期の予測には向かないことが示されている\cite{hafner2019planet}．

二点目について，RNNはモデルの内部状態を十分に更新した後でないと適切に予測が行えず，予測を始める前に{\bf 文脈}としてそれより前の数フレームを与える必要があり，この文脈として与えられるフレーム数が少ないと予測が悪化することが知られている\cite{villegas2019high}．ロボット実機への応用を考えた場合，現在の状態から未来を予測する際に文脈を得るために先に数ステップ行動することは，予測してから行動するという目的意識に反しており実用的できでない．このためRNNベースの手法をロボット実機に応用する際には，文脈として現在の観測という1フレームのみ与れば十分機能するように改善する必要がある．
このように，RNNベースの手法は制約がありそもそも実ロボットへの応用に向いていない可能性がある．

一方，DSSMは各時刻の状態をベクトル(状態ベクトル)で表現し，毎時刻ロボットの行動によって状態ベクトルが遷移し，その時刻に観測される画像は状態ベクトルからの写像であると考えて遷移モデルと写像のモデルを学習する．DSSMは強化学習の分野で長期の予測にも用いられているなど安定した未来の予測に長けているがRNNと比較して画像の生成時に直前の画像を用いないことから高精度な生成は難しく，また映像生成自体を目的にしてDSSMを用いた研究は現状少ない．

\section{本研究の目的}

これらの研究背景を踏まえ，行動条件付き映像予測の問題をより複雑な問題設定にスケールさせることを目指し，本研究では特に実ロボットへの応用を重要視してDSSMベースの行動条件付き映像予測に取り組む．まずDSSMで高精度な生成が難しいことを確認しその理由を簡単に考察する．その上で特に複雑な問題設定に広く取り入れることが可能な帰納バイアスを提案しモデルに組み込むことでDSSMを拡張手法を提案する．さらに行動条件付き映像予測用のデータセットを用いて提案手法の有効性についての定性的・定量的な評価を行い，SSMを使った際にもより高精度な映像予測を可能にすることを目指す．

最後に実験結果を踏まえて，今後の課題と社会応用について述べる．

\section{本論文の構成}



% 多様な環境でさまざまなタスクの遂行が可能な汎用ロボット(generalist robots)の開発は，ロボット工学の最重要課題の一つである．
% ロボットハードウェアの低価格化，汎用的なロボットソフトウェアの普及に加え，近年の急速な深層学習技術の発展を受けて，ロボットの制御方策を自ら学習させる{\bf ロボット学習}の研究が進んでおり，少しずつ遂行可能なタスクが増えている．
% ロボット学習は問題設定によって様々な手法があるが，例えば強化学習をベースにして複雑な形状の物体の把持方策を(grasp2vec)学習するものや，模倣学習によって人のデモンストレーションを真似て食器の出し入れを学習するもの(TCN)，更にはDNNベースを含め様々な手法を統合して家庭用ロボットによる片付け \cite{hatori2018interactively}を遂行する手法などが提案されている．このように実用的なタスクも解決できつつあり，自動化が進む社会においてその活躍の期待値が高まっている．
% 将来の予測がしたい．人間も予測されたものを見て評価したい．
% タスクが難しくなるに連れて？

% 長期の行動系列を
% 長期の予測に基づいて

% \subsection{ロボット学習における将来予測}
% 特に深層強化学習では，，将来の予測がしたい．人間も予測されたものを見て評価したい．

% \subsection{複雑環境に対する状態表現学習}
% (https://arxiv.org/pdf/1802.04181.pdf)データを特徴づける情報を抽出する
% DNNは特徴抽出が得意．教師なし学習で表現学習と呼ばれる．時間変化する環境に拡張した表現学習のことで，各時刻の生の観測データやその系列からこの状態表現(特徴)を見つけることが目的となる．

% planet dreamer
% 上記のような強化学習や実機タスクへの応用から(行動条件付き)映像予測だけを切り取った研究も多い．

% \section{本研究の目的}


% 提案手法の有効性について定性的・定量的な評価を行う．

% 最後に実験結果を踏まえて，今後の課題と社会応用について述べる．

% \section{本論文の構成}
% 本論文の構成は以下のとおりである．


% では
% 第\ref{chap:prerequisite}章では，本研究の前提となる深層生成モデルやメタ学習についての知識を述べる．

% 第\ref{chap:meta_gqn}章では，生成クエリネットワークの確率モデルについてメタ学習のフレームワークを用いて考察を行い，問題点を指摘する．

% %第\ref{chap:related}章では，を行う．
% 第\ref{chap:proposal}章では，前章の議論を踏まえて，生成クエリネットワークを改善する提案手法について説明する．

% 第\ref{chap:experiment}章では，提案手法に対する評価実験の結果を示し，実験結果への考察を行う．

% 第\ref{chap:discussion}章では，本研究の今後の課題と具体的な社会応用可能性について議論する．

% 第\ref{chap:conclusion}章を本論文のまとめとする．